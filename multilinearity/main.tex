\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}

\title{Multilinearity and the Determinant}
\date{\today}
\author{Jason Kenyon}

\begin{document}
\maketitle
\section{Multilinearity}
Here, we define the determinant as the unique multilinear function that is alternating, and one when given the identity matrix.
We then deduce some familiar properties thereof. Finally, we investigate other familiar multilinear functions, as well as introduce sesquilinearity---(multi)linear(ish) functions. 

Henceforth, arguments of functions will be assumed to be vectors, not scalars, of an arbitrary space over an arbitrary field,
unless we state otherwise. 
To begin, we define the determinant of an $n \times n$ matrix $A$.
\begin{defn}
$det(A)=\sum\limits_{i=1}^{n}(-1)^iA_{ij}det(A_{*ij})$ where $A_{*ij}$ is the matrix found by eliminating the $i^{th}$ row and $j^{th}$ column of A.
\end{defn}
The above is the standard computational definition of the determinant, from which its main properties may be derived. We shall have recourse to these properties without proving them. In particular, multiplicativity shall be used extensively. The reader is assumed to be familiar with the basics of the determinant.
\begin{defn}
An n-linear function $f:X \to Y$ is a function such that for all $1\leq i\leq n$ 
$$f(x_1, \dots, ax+y, \dots, x_n)=af(x_1, \dots, x, \dots, x_n)+f(x_1,\dots, y,\dots,x_n)$$
where $ax+y$ is the $i^{th}$ argument of $f$.
\end{defn}
Now that we have our primary object of concern, let's estalish the properties our function must have:
\begin{defn}
An n linear function $f:X \to Y$ is alternating if $$f(x_1, \dots, x_n)=0$$ whenever $x_i=x_{i+1}$ for $1\leq i\leq n$
\end{defn}
Furthermore, suppose that 

$f(x_1, \dots, x_n)=1$ whenever $\begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix}=I$

\begin{thm}
Suppose $f$ satisfies the above properties. Let $A= \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix}$ and $B=\begin{pmatrix}
x_1 \\
\vdots \\ 
x_{i+1} \\ 
x_{i} \\ 
\vdots \\ 
x_n
\end{pmatrix}$
for $1\leq i\leq n$. Then $f(B)=-f(A)$. 
\end{thm}
\begin{proof}
By the alternating condition and multilinearity of $f$, we have 
\begin{align}
0&=f\begin{pmatrix}x \\ \vdots \\ x+y \\ x+y \\ \vdots \\ x_n \end{pmatrix} \\
&=f\begin{pmatrix} x_1 \\ \vdots \\ x \\ x \\ x_n\end{pmatrix} + f\begin{pmatrix} x_1 \\ \vdots \\ y \\ y \\ x_n\end{pmatrix} + f(B)+f(A) \\ 
&=f(B)+f(A)
\end{align}	
\end{proof}
\begin{thm}
Let $A=\begin{pmatrix}x_1 \\ \vdots \\ x_n\end{pmatrix}$ with $x_i=x_j$ for $1\leq i,j \leq n$ and $i\neq j$. Then $f(A)=0$
	
\end{thm}
\begin{proof}
	Suppose that $x_i=x_j$ and $i\neq j$. By the previous theorem, \\ $-f\begin{pmatrix}x_1 \\ \vdots \\ x_{i} \\ x_{i+1} \\ \vdots \\ x_j \\ \vdots \\ x_n \end{pmatrix}=f\begin{pmatrix}x_1 \\ \vdots \\ x_i \\ x_{j} \\ \vdots \\ x_{i+1} \\ \vdots \\ x_n \end{pmatrix}=0$	
\end{proof}
\begin{thm}
Let $A=\begin{pmatrix}x_1 \\ \vdots \\ x_n	\end{pmatrix}$ and $B=\begin{pmatrix}x_1 \\ \vdots \\ x_i+ax_j \\ \vdots \\ x_n \end{pmatrix}$
for $1 \leq j \leq n$.
	
\end{thm}
\begin{proof}
By the previous theorem and multilinearity of $f$:
\begin{align}
f(B)&= f(A)+af\begin{pmatrix}x_1 \\ \vdots \\ x_j \\ \vdots \\ x_j \\ \vdots \\ x_n\end{pmatrix} \\ 
&=f(A)
\end{align}.
\end{proof}
\begin{thm}
Let $A=\begin{pmatrix}x_1 \\ \vdots \\ x_n\end{pmatrix}$. If $rank(A)<n$, then $f(A)=0$.
	
\end{thm}
\begin{proof}
Since $rank(A)<n$ we may deduce $f(A)=f\begin{pmatrix}e_1 \\ \vdots \\ 0 \\ \vdots \\ 0\end{pmatrix}$ with zero vectors beginning at the $rank(A)^{th}$ row. Therefore, we may rewrite $$f(A)=\begin{pmatrix}e_1 \\ \vdots \\ e_{rank(A)+i} - e_{rank(A)+i} \\ \vdots \\ e_n-e_n \end{pmatrix}$$ for $0 \leq i \leq n-rank(A)$. By multilinearity, it is clear that we may decompose this into permutations $f(A)=\sum\limits_{i=1}^{n-rank(A)}f\begin{pmatrix}e_1 \\ \vdots \\ e_{rank(A)+i} \\ \vdots \\ e_n \end{pmatrix}-f\begin{pmatrix}e_1 \\ \vdots \\  e_{rank(A)+i} \\ \vdots \\ e_n \end{pmatrix}$=0
	
\end{proof}
\begin{thm}
	For elementary matrices $E_1, E_2, \text{and } E_3$ of type 1,2, and 3 respectively $f(E_1)=-1$, $f(E_2)=k$, \text{and } $f(E_3)=1$.
\end{thm}
Each of the above follow directly from the multilinearity of $f$ and the assumption that $f(I)=1$. Hence, we shall omit the proof.
\begin{thm}
f(AB)=det(A)f(B).	
\end{thm}
\begin{proof}
We start by proving the theorem for elementary matrices then extend that to arbitrary ones. 
By the previous theorem, $f(E_1)=-1$ and $f(E_1A)=-f(A)$ by theorem 1. Thus, $f(E_1A)=det(E_1)f(A)$ since $E_1$ is the result of swapping a row in the identy matrix. Next, $f(E_2)=k$ by the previous theorem and $det(E_2)=kdet(I)=k$. Since $f(E_2A)=kf(A)$, we have $f(E_2A)=det(E_2)f(A)$. Finally, by the previous theorem, we have $f(E_3)=1$ and $det(E_3)=det(I)+kdet(E)$ for some matrix $E$ with two equivalent rows. Therefore, $f(E_3A)=f(A)=det(E_3)f(A)$.

	We know that $rank(AB)\leq rank(A)$ by definition of matrix multiplication, and $f(A)=0 \text{ if } rank(A)<n$ by theorem 4. Thus if $rank(A)<n$, 
$f(AB)=0$ and similarly, $det(A)=0$. So $f(AB)=det(A)f(B)=0$. 
If $A$ is full rank, then we may represent $A=E_1E_2\dots E_n$ as full rank implies that $A$ may be completely row reduced. This means that 
\begin{align}
f(AB)&=f(E_1\dots E_nB) \\ 
&=det(E_1)f(E_2\dots E_nB) \\ 
&= \dots \\
&=det(E_1)\dots det(E_n)f(B) \\ 
&=det(E_1\dots E_n)f(B) \\ 
&=det(A)f(B)
\end{align}
\end{proof}
\begin{thm}
f(A)=det(A)	
\end{thm}
\begin{proof}
If $rank(A)<n$ then $f(A)=0$ by theorem 4. Similarly, $det(A)=0$. 
Otherwise, just as before we may represent $A=E_1 \dots E_n$. Hence  
\begin{align}
f(A)&=f(E_1\dots E_n) \\
&=det(E_1)f(E_2 \dots E_n) \\
&= \dots \\ 
&=det(E_1)\dots det(E_n) \\ 
&=det(E_1\dots E_n) \\
&=det(A)
\end{align}
\end{proof}
Therefore we have shown that the determinant is nothing but the alternating, multilinear function that evaluates to 1 given the identity.
\section{Inner Products and Definiteness}
Inner products are of the the primary tools used in analysis and geometry. Their properties correspond directly to our idea of space, and are therefore of utmost import.
\begin{defn}
	An inner product is a function $\langle \cdot, \cdot \rangle : V \times V \to F$ for a given vector space $V$ over a field $F$ that satisfies the following:
\begin{enumerate}
\item $\langle v, w \rangle=\overline{\langle w, v \rangle}$
\item $\langle rv+q, w \rangle=r\langle v, w \rangle + \langle q, w \rangle$
\item $\langle v, v \rangle>0$
\end{enumerate}
	for all $v,w,q \in V$ and $r \in F$, with $v \neq \bold{0}$.  
\end{defn}
Note that $F$ is either $\mathbb{R}$ or $\mathbb{C}$. Also, if $F=\mathbb{R}$ then linearity in the first argument, coupled with symmetry, implies linearity in the second. The standard inner product on $\mathbb{C}^n$ is defined as follows:
\begin{defn}
	$ v \cdot w =\sum\limits_{i=1}^n \bar{v_i}w_i$ 
\end{defn}
As the conjugate of a real number is itself, this definition reduces to the familiar dot product on $\mathbb{R}^n$. Additionally, it should be clear that this definition satisfies the criteria defined previously for inner products. 
Now we define what it means for a matrix to be positive definite: 
\begin{defn}
	An $n \times n$  matrix $A$ with entries in either $\mathbb{R}$ or $\mathbb{C}$ is called positive definite if 
$v^*Av > 0$ for all nonzero $v \in F^n$. 
\end{defn}
Now we show that any positive definite $n \times n$ self-adjoint matrix defines an inner product. 
\begin{thm}
	Let $F$ be either the real or complex field and $A$ an $n \times n$ matrix with entries thereof. If $A^*=A$, then $\langle x, y \rangle:=x^*Ay$ is an inner product.
\end{thm}
\begin{proof}
We begin by demonstrating conjugate symmetry:
\begin{align}
	\langle x, y \rangle &= x^*Ay \\
	&= x \cdot Ay \\
	&= \overline{Ay \cdot x} \\
	&= \overline{(Ay)^*x} \\
	&= \overline{y^*A^*x} \\
	&= \overline{y^*Ax} \\
	&= \overline{\langle y, x \rangle}
\end{align}
Next, linearity in the first argument:
\begin{align}
	\langle ax+b, y \rangle &= (ax+b)^*Ay \\
	&= ((ax)^*+b^*)Ay \\ 
	&= (ax)^*Ay +b^*Ay \\ 
	&= \bar{a}x^*Ay + \langle b, y \rangle \\
	&= \bar{a}x \cdot Ay + \langle b, y \rangle \\
	&= (\bar{a}x)^*Ay + \langle b, y \rangle \\
	&= x^*aAy + \langle b, y \rangle \\
	&= ax^*Ay + \langle b, y \rangle \\
	&= a \langle x, y \rangle + \langle b, y \rangle 
\end{align}
And finally,
$\langle x, x \rangle =x^*Ax>0$ when $x$ is nonzero, by assumption. Furthermore, $\bold{0}A \bold{0}=0$. Thus 
$\langle x, x \rangle =0$ if and only if $x=\bold{0}$.
\end{proof}
Additionally, it should be easy to see that conjugate symmetry and linearity in the first argument imply that 
$\langle x, ay+b \rangle=\bar{a} \langle x, y \rangle + \langle x, b \rangle$ when $F \neq \mathbb{R}$. That is, the inner product is sesquilinear in general, but reduces to a bilinear function in the case of $\mathbb{R}$.

With this result now proven, we see that the standard inner product on $\mathbb{C}^n$ is the inner product defined by the identity matrix.
\end{document}
