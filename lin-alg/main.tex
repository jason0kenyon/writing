\documentclass[oneside, 12pt]{book}
\usepackage{indentfirst}
\usepackage{amsthm, amsmath, amssymb, amsfonts}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\im}{im}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black,
    bookmarks=true,
  }
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{lem}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\renewcommand\qedsymbol{$\blacksquare$}
\title{Finite Dimensional Inner Product Spaces}
\author{Jason Kenyon}
\date{November 2022}

\begin{document}
\frontmatter
\maketitle
\cleardoublepage
\pdfbookmark{\contentsname}{toc}
\tableofcontents
\chapter*{Preface}
Hello
\addcontentsline{toc}{chapter}{Preface}
\mainmatter
\chapter{Vector Spaces}
\section{test}
\begin{thm}
test
\end{thm}
\chapter{Linear Functions}
\chapter{Linear Systems of Equations}
\section{Rank}
\begin{defn}
\label{defn_elmop}
  An elementary row or column operation on an $m \times n$ matrix $A$ is defined as one of the following:
  \begin{enumerate}
    \item Interchanging any two rows or columns of $A$
    \item Scaling each entry in a row or or column of $A$
    \item Adding a multiple of one row or column to another row or column of $A$
  \end{enumerate}
  An elementary matrix is the result of applying one of the above to the $n \times n$ identity matrix.
\end{defn}
\begin{thm}
\label{thm_elmop}
  Suppose that $B$ is the result of applying an elementary row operation to $A$. Then there exists an elementary matrix $E$ such that $B=EA$. Furthermore, $E$ is the matrix obtained by performing the same elementary row operation to $I_{n}$ as was performed to convert $A$ into $B$. Similarly, if $B$ is the result of applying
  an elementary column operation to $A$, then there exits an elementary matrix $E$ such that $B=AE$, and $E$ is the result of applying the same elementary column operation to $I_{m}$ as was applied to $A$.
\end{thm}
The proof is a tedious verification of cases; the elementary matrices are defined precisely for this to work.
\begin{defn}
\label{defn_rnk}
The rank of a matrix $A$ is defined as the rank of the linear function $L_{A}=Ax$
\end{defn}
\begin{thm}
\label{rnkeq}
  Let $T: V \to W$ be linear and $A=[T]_{\beta}^{\gamma}$. Then $\rank(T)=\rank(L_{A})$
\end{thm}
\begin{proof}
  Consider the map $\phi_{\beta}:V \to \mathbb{F}^{n}$. That is, the function mapping a vector to its representation in coordinates. This is linear by definition and invertible as we know that any basis represents a vector uniquely as a linear combination of its elements. We have
  \[L_{A}(\mathbb{F}^{n})=L_{A}\phi_{\beta}(V)=\phi_{\gamma}(T(V)).\] It follows that \[\dim(\im(L_{A}))=\dim(\im(T))\] because $\phi_{\gamma}$ is an isomorphism.
\end{proof}
\begin{thm}
\label{thm_rnkprp}
  Let $A$ be an $m \times n$. Let $P$ and $Q$ be invertible $m \times m$ and $n \times n$ matrices, respectively.
  Then
  \begin{enumerate}
    \item $\rank(AQ)=\rank(A)$
    \item $\rank(PA)=\rank(A)$
    \item $\rank(PAQ)$
  \end{enumerate}
  \begin{proof}
    \begin{align}
      \im(L_{AQ})&=\im(L_{A}L_{Q}) \\
                 &= L_{A}L_{Q}(\mathbb{F}^{n}) \\
                 &= L_{A}(L_{Q}((\mathbb{F}^{n})) \\
                 &= L_{A}(\mathbb{F}^{n}) \\
                 &=\im(L_{A})
    \end{align}
    Thus, $\rank(L_{AQ})=\rank(L_{A})$.
    Similarly, $\im(L_{P}L_{A})=L_{P}(\im(L_{A}))=\im(L_{A})$
    and so $\dim(\im(L_{P}L_{A}))=\dim(\im(L_{A}))$ since $P$ is an isomorphism.
    It follows, by applying the previous two results that
    $\rank(PAQ)=\rank(A)$.
  \end{proof}
\end{thm}
\begin{thm}
\label{thm_colm}
Let \[A=\begin{pmatrix}
          a_{11} & \cdots & a_{1n} \\
          \vdots & \ddots & \vdots \\
          a_{1m} & \cdots & a_{mn}

        \end{pmatrix}.\] Then $\rank(A)=\dim \left(
        \spn{ \left \{
          \begin{pmatrix} a_{11} \\ \vdots \\ a_{1m}\end{pmatrix}, \cdots \begin{pmatrix} a_{1n}\\ \vdots \\ a_{mn} \end{pmatrix}
        \right \}
        }\right )
        $
\end{thm}
\begin{proof}
  \begin{align}
    \im(L_{A}) &= L_{A}(\mathbb{F}^{n}) \\
               &= L_{A}(\spn{\{e_{1}, \dots e_{n}\}}) \\
               &= \spn{\{Ae_{1}, \dots, Ae_{n}\}} \\
    &=
        \spn{ \left \{
          \begin{pmatrix} a_{11} \\ \vdots \\ a_{1m}\end{pmatrix}, \cdots \begin{pmatrix} a_{1n}\\ \vdots \\ a_{mn} \end{pmatrix}
        \right \}
        }
  \end{align}
  Furthermore, $\dim(\spn{(X)})$ is nothing but the number of linearly independent vectors in $X$ for any set of vectors $X$. Thus we have shown
  that the rank of a matrix is nothing but the number of linearly independent vectors in its columns.
\end{proof}
\begin{thm}
\label{thm_rrefrnk}
Let $A$ be an $m \times n$ matrix. Then a finite composition of elementary row and column operations applied to $A$ results in a matrix
of the form
\[ \begin{pmatrix}
     I_{\rank(A)} & O_{1} \\
     O_{2} & O_{3}
   \end{pmatrix}
 \]
 where $O_{1}, O_{2}, O_{3}$ are zero matrices.
\end{thm}
\begin{proof}
  First, note that if $A$ is a zero matrix, then by theorem \ref{thm_colm} $\rank(A)=0$, and so $A=I_{0}$, the degenerate case of our claim.
  Suppose otherwise.
  We proceed by induction on $m$, the number of rows of $A$. In the case that $m=1$, we may convert $A$ to a matrix of the form
  \[\begin{pmatrix}
      1 & 0 & \cdots & 0
    \end{pmatrix}\] by first making the leftmost entry $1$ and adding the corresponding additive inverses of the others to the other columns.
  Clearly the rank of the above matrix is $1$ and is of the form
  \[\begin{pmatrix}
I_{1} & O
    \end{pmatrix}\]
  This is another degenerate case, as it lacks zeros below the identity.
  Now suppose that our theorem holds when $A$ has $m-1$ rows.

  To demonstrate that our theorem holds when $A$ is an $m \times n$ matrix, notice that when $n=1$, we can argue that our theorem holds as before, but
  using row operations instead of column operations. This is another degenerate case. For $n>0$, note that there exists an entry $A_{ij}\neq0$
  and by applying at most an elementary row and column operation, we can move $A_{ij}$ to position $1,1$. Additionally, we may transform $A_{ij}$ to
  value $1$, and as before, transform all of the entries in row and column 1 besides $A_{ij}$ to $0$. Thus we have a matrix of the form
  \[\begin{pmatrix}
      1 & 0 & \cdots & 0 \\
      0 & x_{11} & \cdots & x_{1 \ n-1}       \\
      \vdots & \vdots & \ddots &  \vdots  \\
      0 & x_{m-1 \ 1}& \cdots & x_{m-1 \ n-1}
    \end{pmatrix}\]
\end{proof}
The submatrix defined by $x_{ij}$ is of dimension $m-1 \times n-1$ and so must have rank $\rank(A)-1$ as elementary opertations preserve rank and deleting a row and column of a matrix reduces its rank by 1. Furthermore, by our induction hypothesis the above matrix may be converted via a finite number
of elementary operations to a matrix of the form
\[\begin{pmatrix}
    1 & 0 & \cdots & 0 \\
    0 & I_{\rank(A)-1} & O_{1} \\
    \vdots &  &   \\
    0 & O_{2} & O_{3}
  \end{pmatrix}\]
Therefore, for an $m \times n$ matrix $A$, a finite number of elementary operations converts it into a matrix of the form
\[\begin{pmatrix}
    I_{\rank(A)} & O_{1} \\
    O_{2} & O_{3} \\
  \end{pmatrix}\].
\begin{thm}
\label{thm_colrow}
  For any matrix $A$, $\rank(A^{T})=\rank(A)$.
\end{thm}
\begin{proof}
  By theorem \ref{thm_rrefrnk}, we may convert $A$ to a matrix $D=BAC$ where $B=E_{1}\cdots E_{p}$ and $C=G_{1}\cdots G_{q}$ where $E_{i}$ and $G_{i}$ are elementary row and column matrices respectively. It follows that $D^{T}=C^{T}A^{T}B^{T}$, whence $\rank(A^{T})=\rank(D^{T})$ by theorem (insert) because elementary
  matrices are invertible, and so is the transpose of the compositions thereof. Further, $D^{T}$ must be of the same form as $D$ since the only nonzero entries of $D$ are along the diagonal from entry $1,1$ to entry $\rank(A), \rank(A)$. Hence, we have $\rank(A)$ linearly independent columns in the matrix $D^{T}$.

  Since the columns of $D^{T}$ are the rows of $D$, we see that the number of linearly independent columns of $A$ is equal to the number of linearly
  independent columns of $A^{T}$. In other words, the dimension of the space generated  by the columns of $A$ is equal to the dimension of the space generated by its rows.
\end{proof}
\begin{thm}
\label{thm_invm}
  Let $A$ be an invertible $n \times n$ matrix. Then $A$ is a product of elementary matrices.
\end{thm}
\begin{proof}
  By the dimension theorem, if $A$ is invertible,  then $\rank(A)=n$. So by theorem \ref{thm_rrefrnk} $A$ may converted into a matrix of the form
  $I_{n}=E_{1}\cdots E_{p}AG_{1} \cdots G_{q}$, whence $A=E_{1}^{-1}\cdots E_{p}^{-1}I_{n}G_{1}^{-1}\cdots G_{q}^{-1}$.
\end{proof}
\begin{thm}
\label{thm_rnkinq}
  Let $T:V \to W$ and $U:W \to Z$.  Then
  \begin{enumerate}
    \item $\rank(TU)\leq \rank(U)$
    \item $\rank(TU) \leq \rank(T)$
  \end{enumerate}
\end{thm}
\begin{proof}
  We have
  \begin{align}
    \rank(TU)&=\dim(\im(TU)) \\
            &= \dim(\im(T(U(V)))) \\
            &\subseteq U(W) \\
            &= \im(U)
  \end{align}
  Therefore, $\dim(\im(TU))\leq \dim(\im(U))$. Next, let $\beta, \gamma, \phi$ be ordered bases for $V, W, $ and $Z$, respectively; and let
  $A=[T]_{\beta}^{\gamma}$ and $B=[U]_{\gamma}^{\phi}$.
  By theorem \ref{thm_colrow}
  \begin{align}
    \dim(\im(TU))&=\dim(\im(AB)) \\
                 &= \dim(\im((AB)^{T}) \\
                 &= \dim(\im(B^{T}A^{T})) \\
                 &\leq \dim(\im(A^{T})) \\
                 &= \dim(\im(A)) \\
    &= \dim(\im(T))
  \end{align}
\end{proof}
\section{Form}
We now apply the fruits of our investigation into vector spaces and linearity to solve systems of linear equations.
\begin{defn}
\label{def_linsys}
  A linear system of equations is a collection of $m$ equations of the form:
  \[ a_{1}x_{1}+ \cdots +a_{n}x_{n}=b \] where $a_{i}, x_{i},b \in \mathbb{F}$ for $1 \leq i \leq n$.
  Equivalently, we may say
  $Ax=b$ for an $m \times n$ matrix $A$, where $x= \begin{pmatrix} x_{1} \\ \vdots \\ x_{n}\end{pmatrix}$ and $b= \begin{pmatrix} b_{1} \\ \vdots \\ b_{m}\end{pmatrix}$.
  If $b=\bold{0}$, the linear system is said to be homogenous.
\end{defn}
\begin{defn}
\label{def_soln}
A solution to a linear system is a vector $s \in \mathbb{F}^{n}$ such that $As=b$
\end{defn}

\begin{thm}
\label{thm_homsoln}
Let $A$ be an $m \times n$ matrix over $\mathbb{F}$. If $m<n$, then the homogenous system $Ax=0$ has a nontrivial solution.
\end{thm}
\begin{proof}
  Notice that, the solution set to the system $Ax=0$ is $\ker(L_{A})$, so by the dimension theorem, $\dim(\ker(A))=n-\rank(L_{A})$. Additionally,
  we know that $\rank(A)$ is nothing but the number of linearly independent vectors defined by its rows which certainly cannot exceed $m$. Therefore
  $\rank(A) \leq m < n$, in which case $n-\rank(A)=\dim(\ker(A))>0$, and so $\ker(A)\neq \{0 \}$.
\end{proof}
\begin{thm}
\label{thm_gensoln}
For any solution $s$ to the linear system $Ax=b$, \[\{s+s_{0}:As_{0}=\bold{0} \}\] is its solution set.
\end{thm}
\begin{proof}
Suppose that $As=b$ and $As'=b$. Then $A(s'-s)=As'-As=b-b=0$. It follows that $s+(s'-s) \in S$. Conversely, if $y \in S$, then $y=s+s'$, in which case $Ay=A(s+s')=As+As'=b+0=b$. That is, $Ay=b$.
\end{proof}
\begin{thm}
\label{thm_uniqsoln}
  Let $Ax=b$ for an $n \times n$ matrix $A$. If $A$ is invertible, then the system has a single solution $A^{-1}b$. If the system has a single solution, then $A$ is invertible.
\end{thm}
\begin{proof}
Suppose $A$ is invertible. Then $A(A^{-1}b)=AA^{-1}(b)=b$. Furthermore, if $As=b$ for some $s \in \mathbb{F}^{n}$, then $A^{-1}(As)=A^{-1}b$ and so $s=A^{-1}b$.
Next, suppose that the system has a unique solution $s$. Then by theorem \ref{thm_gensoln}, we know that the solution set $S=\{s+s_{0}: As_{0}=0 \}$. But this is only the case if $\ker(A)=\{0\}$, lest $s$ not be unique. And so, by the dimension theorem, $A$ is invertible.
\end{proof}
\begin{thm}
\label{thm_nonemsoln}
The linear system $Ax=b$ has a nonempty solution set if and only if $\rank(A)=\rank(A|b)$.
\end{thm}
\begin{proof}
  If the system has a solution, then $b \in \im(L_{A})$. Additionally, $\im(L_{A})=L_{A}(F^{n})$ and $L_{A}(e_{i})=Ae_{i}=
  \begin{pmatrix} a_{1i} \\ \vdots \\ a_{ni}   \end{pmatrix}$. Therefore, since $L_{A}(\mathbb{F}^{n})= \spn \{Ae_{1}, \dots Ae_{n}\}$,
  $\im(L_{A})=\spn\{A_{1}, \dots A_{n}\}$, where $A_{i}$ is the $i^{th}$ column of $A$.
  Certainly, $b \in \spn\{A_{1}, \dots A_{n}\}$ if and only if $\spn\{A_{1}, \dots A_{n}\}=\spn\{A_{1}, \dots A_{n}, b\}$, which is to say
  $\dim(\im(\spn\{A_{1}, \dots A_{n}\}))=\dim(\im(\spn\{A_{1}, \dots A_{n}, b\}))$, or, $\rank(A)=\rank(A|b)$.
\end{proof}
\begin{cor}
\label{cor_infsoln}
Let $Ax=b$ be a linear system of $m$ equations in $n$ variables. Then its solution set is either, empty, of one element, or of infinitely many elements (provided that $\mathbb{F}$ is not a finite field).
\end{cor}
\begin{proof}
  By theorem \ref{thm_nonemsoln} $Ax=b$ has a nonempty solution set if and only if $\rank(A)=\rank(A|b)$. Therefore, it may be that our linear system has no solutions; however, supposing that this is not the case, by theorem \ref{thm_uniqsoln} it has a unique solution if and only if $A$ is invertible. Finally, assume that our linear system has neither no solution nor a single solution. This yields
  \begin{equation}
Ax_{1}=Ax_{2}=b
\end{equation}
for $x_{1}, x_{2} \in \mathbb{F}^{n}$, which implies
\begin{align}
  Ax_{1}-Ax_{2}&=\bold{0} \\
               &=A(x_{1}-x_{2}) \\
               &= nA(x_{1}-x_{2}) \\
               &= A(n(x_{1}-x_{2})) \\
\end{align}
where $n \in \mathbb{F}$.
Thus, by theorem \ref{thm_gensoln}
\[A(x_{1}+n(x_{1}-x_{2}))=b.\]
\end{proof}
\section{Solution}
\begin{defn}
\label{defn_rref}
A matrix of the form \[\begin{pmatrix} a_{11} & \cdots & a_{1n}\\ \vdots & \ddots & \vdots \\a_{m1} & \cdots & a_{mn}\end{pmatrix}\] is said to be in reduced echelon form if
\begin{enumerate}
  \item $a_{ii}\neq 0$ implies that $a_{ij}=1$
  \item $a_{ij} \neq 1$ implies that $a_{ij}=0$
  \item $a_{ij}=0$ for all $1\leq j \leq n$ implies that $i < r$ for all nonzero rows $\begin{pmatrix}a_{r1} & \cdots & a_{rn}\end{pmatrix}$
\end{enumerate}
\end{defn}
\begin{thm}
\label{thm_rref}
Any matrix can be converted into reduced echelon form via a finite number of elementary row operations.
\end{thm}
\begin{proof}
This is a restatement of theorem \ref{thm_rrefrnk}.
\end{proof}
This form is of particular interest because reducing an augmented matrix is equivalent to solving a linear system of equations. We now have a
procedure for solving arbitrary systems of linear equations. For example, we may now demonstrate that a set of vectors is linearly dependent by
finding a nontrivial solution to a linear system of equations; similarly we may apply theorem \ref{thm_nonemsoln} to demonstrate that a set of vectors is linearly dependent. In the following chapter, we will also see that computing the elements of an eigenspace is made possible by reducing a matrix.
It follows that
\begin{cor}
\label{cor_soln}
For any invertible $n \times n$ matrix $A$.
\[A^{-{1}}(A|I_{n}) = E_{1} \cdots E_{p}(A|I_{n})=(I_{n}|A^{-1})\]
where $E_{1}, \dots, E_{p}$ are elementary matrices.
\end{cor}
Notice that the above elementary matrices may be either row or column matrices; however, since we are left multiplying, the product will result in a
row operation.
Thus we now have a procedure for finding the inverse of any matrix: perform row operations to convert it into the identity matrix, while accounting for each change.
Additionally,
\begin{cor}
\label{cor_solninvar}
Let $A$ be an $m \times n$ matrix and $C$ be an invertible $n \times n$ matrix. Then the solutions sets to the linear systems
\[Ax=b \text{and} CAx=Cb \] are equal.
\end{cor}
This follow directly from the invertibility, and fits with our intuition: as we row reduce a linear system, its solutions do not change.
\chapter{Eigenspaces}
\chapter{Orthogonality}
\section{Inner Products}
Hello
\section{Projections}
\begin{defn}
\label{defn_prj}
Let $V=W_{1}\oplus W_{2}$. A projection of $V$ on $W_{1}$ along $W_{2}$ is a linear function $T:V \to V$ such that for any $x \in V$ where $x=x_{1}+x_{2}$ $x_{1} \in W_{1}$ and $x_{2} \in W_{2}$ $T(x)=x_{1}$.
\end{defn}
\begin{thm}
\label{projiff}
A linear function $T:V \to V$ is a projection of $V$ on $W_{1}=\{x : T(x)=x \}$ along $\ker{T}$ if and only if $T=T^{2}$.
\end{thm}
\begin{proof}
Trivial
\end{proof}
\section{Orthogonal Projection}
\begin{defn}
\label{orthcomp}
Let $W \subseteq V$. The orthogonal complement of $W$ is defined as $W^{\perp}=\{v \in V : \langle v, w \rangle=0 for all w \in W\}$.
\end{defn}
\begin{thm}
The following statments are true
\begin{enumerate}
  \item $W^{\perp}$ is a subspace of $V$
  \item $\dim(W^{\perp})=\dim(V)-\dim(W)$
\end{enumerate}
\end{thm}
\begin{proof}
Trivial
\end{proof}
\begin{thm}
\label{thm_orthodecomp}
  Let $W \subseteq V$.Then for any $x \in V$ there exist unique vectors $y \in W$ and $z \in W^{\perp}$ such that $x=y+z$. Furthermore, for all $w \in W$
s  \[ ||y-x|| \leq ||w-x||\] and we call $y$ the orthogonal projection of $z$ on $w$, denoted $x_{w}$. Similarly, $z$ is denoted $x_{\perp}$.
\end{thm}
\begin{proof}
trivial
\end{proof}

\begin{thm}
Let $W \subseteq V$ $x \in V$ and $\beta=\{v_{1}, \dots v_{n}\}$ be an orthonormal basis for $W$ and $A$ be the matrix whose $j^{th}$ column is $v_{j}$. Then the orthogonal projection of $x$ on $W$ $x_{w}=AA^{*}x$.
\end{thm}

\begin{proof}
  We begin by demonstrating that $W^{\perp}=\ker A^{*}$. We have
  \[A^{*}x= \begin{pmatrix}v_{1}^{*}x \\ \vdots \\ v_{n}^{*}x \end{pmatrix}=\begin{pmatrix}\langle v_{1}, x \rangle \\  \vdots \\ \langle v_{n}, x \rangle\end{pmatrix}.\] Certainly
  $Ax=\bold{0}$ if and only if $\langle v_{i}, x \rangle=0$ for all $1 \leq i \leq n$. But that is to say $x \in W^{\perp}$, and so \[\ker(A^{*})=W^{\perp}.\]
  Note that $Ax=\spn\beta$ by definition. Therefore, for some $c \in \mathbb{F}^{n}$ $Ac=x_{w}$, which means that $x-x_{W}=x-Ac \in W^{\perp}$. It follows that
  $A^{*}(x-Ac)=0$ and so \[A^{*}Ac=A^{*}x.\] Thus, if we see that $x_{w}=Ac$. Furthermore, since $\beta$ is orthonormal, $A$ must be unitary, in which case \[Ac=AA^{*}x=x_{W}.\]
\end{proof}
\begin{cor}
  \label{cor_orthproj}
$A^{*}A$ is a projection and $\im(A^{*}A)^{\perp}=\ker(A^{*}A)$.
\end{cor}
\begin{proof}
trivial
\end{proof}
\begin{defn}
  There exists
\end{defn}
\appendix
\chapter{Determinants as Permutations}
Hello







\end{document}
