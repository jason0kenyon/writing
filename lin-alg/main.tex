\documentclass[oneside, 12pt]{book}
\usepackage{indentfirst}
\usepackage[backend=biber]{biblatex}
\addbibresource{refs.bib}
\usepackage{amsthm, amsmath, amssymb, amsfonts}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\im}{im}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black,
    bookmarks=true,
  }
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{lem}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\renewcommand\qedsymbol{$\blacksquare$}
\title{Finite Dimensional Inner Product Spaces}
\author{Jason Kenyon}
\date{November 2022}

\begin{document}
\frontmatter
\maketitle
\cleardoublepage
\pdfbookmark{\contentsname}{toc}
\tableofcontents
\chapter*{Preface}
Hello
\addcontentsline{toc}{chapter}{Preface}
\mainmatter
\chapter{Vector Spaces}
\section{Spaces and Subspaces}
\begin{defn}
  \label{defn_vspace}
  A vector space $V$ over a field $\mathbb{F}$ is a set, along with a binary operation $+: V^{2} \to V$ and a binary operation $\boldsymbol{\cdot}: \mathbb{F} \times V \to V$ that satisfy the following properties:
  \begin{enumerate}
    \item $a\boldsymbol{\cdot}v+w \in V$
    \item $v+w=w+v$
    \item $v+(w+z)=(v+w)+z$
    \item $\bold{1}v=v$
    \item $(a\boldsymbol{\cdot}b)x=a\boldsymbol{\cdot}(bx)$
    \item $a\boldsymbol{\cdot}(v+w)=av+aw$
    \item $(a+b)v=a\boldsymbol{\cdot}v+b\boldsymbol{\cdot}v$
    \item There exists an element $\bold{0} \in V$ such that $v+\bold{0}=v$
    \item There exists an element $v^{-1}$ such that $v+v^{-1}=\bold{0}$
  \end{enumerate}
  for all $a, b \in \mathbb{F}$ and $v,w,z \in V$.
\end{defn}
The elements of $V$ are called vectors, and the elements of $\mathbb{F}$ are called scalars. The operations $+$ and $\boldsymbol{\cdot}$ are called vector addition and scalar multiplication, respectively. We omit the $\boldsymbol{\cdot}$ and do not explicitly apply $+$ for clarity. The $\bold{1}$ is the identity element of $\mathbb{F}$. $-a$ will denote the additive inverse of $a \in \mathbb{F}$ for the field $\mathbb{F}$, and $-v=v^{-1}$ will denote the additive inverse of a vector $v \in V$ under vector addition, while $-a(v)$ will denote multiplication of a vector by a scalar's additive inverse in $\mathbb{F}$.

\begin{thm}
  \label{thm_canc}
  Let $x$, $y$, and $z$ be vectors in $V$. If $x+z=y+z$, then $x=y$.
  The zero element of $V$ is unique.
  The additive inverse in $V$ is unique for each vector in $V$.
\end{thm}
\begin{thm}
\label{thm_alg}

\begin{enumerate}
  \item $0(x)=\bold{0}$
  \item $(-a)x=-(ax)=a(-x)$
  \item $a(\bold{0})=\bold{0}$
\end{enumerate}

\end{thm}
\begin{defn}
\label{defn_subspace}
  A subspace $W$ of a vector space $V$ is a set $W \subseteq V$ that is itself a vector space.
\end{defn}
\begin{thm}
\label{thm_subspace}
  Let $V$ be a vector space with zero element $\bold{0}$. Then a subset $W \subseteq V$
  is a subspace of $V$ if and only if \[\bold{0} \in W\] and \[cx+y \in W\] for all $x,y \in W$ and $c \in \mathbb{F}$.
\end{thm}
\section{Linear Independence}
\begin{defn}
  \label{defn_linind}
  A set of vectors $\{v_{1}, v_{2}, \dots v_{n}\}$ is linearly dependent if
  \[a_{1}v_{1}+a_{2}v_{2}+ \dots a_{n}v_{n}=\bold{0}\] for $a_{1}, a_{2}, \dots a_{n} \in \mathbb{F}$ not all zero. Similarly, a set of vectors is linearly independent if it is not linearly dependent.
\end{defn}
\begin{thm}
  \label{thm_linind}
If $S_{1}\subseteq S_{2} \subseteq V$ and $S_{1}$ is linearly dependent, then $S_{2}$ is linearly dependent as well. Similarly, if $S_{2}$ is linearly dependent, then $S_{1}$ is linearly dependent.
\end{thm}
\begin{proof}
The proof should be clear when considering the above definition.
\end{proof}
\begin{thm}
  \label{thm_span}
Let $S$ be a linearly independent subset of $V$ and $v \in V$ such that $v \in S$. Then $S \cup \{v\}$ is linearly dependent if and only if $v \in \spn(S)$.
\end{thm}
\begin{proof}
  If $S \cup \{v\}$ is linearly dependent then there exist scalars $a_{1}, a_{2}, \dots a_{n}, a_{v} \in \mathbb{F}$ not all zero such that \[a_{1}s_{1} +a_{2}s_{2}+\cdots +a_{n}s_{n}+a_{v}v=\bold{0}.\]
  Therefore, $a_{v}\neq0$, for otherwise we would contradict the linear independence of $S$. This implies that \[v=-\frac{a_{1}s_{1}+ a_{2}s_{2}+ \cdots a_{n}s_{n}}{a_{v}}.\] and hence $v \in \spn(S)$. Conversely, if $v \in \spn(S)$, then \[v=a_{1}s_{1}+a_{2}s_{2}+\cdots a_{n}s_{n} \] for some scalars $a_{1},a_{2}, \cdots \in \mathbb{F}$ This implies that \[1(v)-(a_{1}s_{1}+\cdots a_{n}s_{n})=\bold{0}.\] which is a nontrivial solution, so the set $S \cup \{v\}$ is linearly dependent.
\end{proof}
\section{Bases}
\begin{defn}
  \label{defn_basis}
A subset $\beta \subseteq V $ is a basis for $V$ if it is a linearly independent set such that $\spn(\beta)=V$.
\end{defn}
\begin{thm}
  \label{thm_uniqbas}
A subset $\beta=\{v_{1},v_{2}, \dots v_{n}\}$ of $V$ is a basis for $V$ if and only if for any vector $v \in V$ \[v=a_{1}v_{1}+\cdots a_{n}v_{n}\] for unique scalars $a_{1}, \dots a_{n} \in \mathbb{F}$.
\end{thm}
\begin{proof}
Suppose that $\beta=\{v_{1}, \dots v_{n}\}$ is a linearly independent generating set of $V$. Then $v=a_{1}v_{1}+\cdots a_{n}v_{n}$ for scalars $a_{1}, \dots a_{n} \in \mathbb{F}$. Further, suppose that there exists another collection $b_{1}, \dots b_{n}$ of scalars such that $v=b_{1}v_{1}+\cdots b_{n}v_{n}$. Subtracting, we have \[(a_{1}-b_{1})v_{1}\cdots (a_{n}-b_{n})v_{n}=\bold{0}.\] Since $\beta$ is linearly independent, it follows that $a_{i}-b_{i}=0$, and hence $a_{i}=b_{i}$ for all $1 \leq i\leq n$. Therefore, the linear combination $a_{1}v_{1}+\cdots a_{n}v_{n}$ is the unique representation of $V$ for $\beta$. Similarly, if we know that $v=a_{1}v_{1}\cdots a_{n}v_{n}$ for unique scalars, then \[(b_{1})v_{1}+\cdots(b_{n})v_{n}=\bold{0}=v-v.\] if and only if $b_{i}=a_{i}-a_{i}=0$ for all $1 \leq i \leq n$. And certainly $V=\spn(\beta)$, so $\beta$ is a basis for $V$.
\end{proof}




see this source \textcite{jechset}.
\begin{thm}
\label{thm_basis}
Every vector space has a basis.
\end{thm}
\begin{proof}
  Consider the set $L$ of all linearly independent subsets of a vector space $V$. Let $T \subseteq L$ be a chain. That is, for any two sets $A$ and $B$ in $T$ either $A \subseteq B$ or $B \subseteq A$. Hence, any finite subset of $\bigcup T$ is in $L$. In other words, taking a union over a chain yields an upper bound under $\subseteq$ which must necessarily be in the set from whence it came. This ensures that $T$ is linearly ordered by $\subseteq$, for transitivity, reflexivity, and antisymmetry are already satisfied by definition of a subset. Therefore, Zorn's lemma implies that there exists a maximal element in $L$. That is, there exists an element $l \in L$ such that for all $A \in L$ $A \subseteq l$. Moreover, we know that $l$ is linearly independent by assumption.

  To show that $l$ spans $V$, suppose that there were an element $v \in V$ such that $v\notin \spn(l)$. Then by theorem \ref{thm_span} $l \cup \{v\}$ would be a linearly independent set, in which case $l \cup \{v\} \in L $. But $l \cup \{v \} \nsubseteq l$, contradicting the fact that $l$ is the maximal element of $L$.
\end{proof}
\begin{cor}
  \label{cor_bascard}
If $V$ is generated by a finite set, then there exists a finite basis for $V$ contained within the generating set.
\end{cor}
\begin{proof}
  Suppose that $\spn(S)=V$ for a finite set $S$. Consider an arbitrary linearly independent subset $\beta \subseteq S$ such that $\beta \cup \{v\}$ is linearly dependent for any $v \in S$ such that $v \notin \beta$. Such a set certainly exist because any set containing a single vector is linearly independent, and so we may continue to add vectors from $S$ into $\beta$ until another union results in a linearly dependent set. Hence if we demonstrate that $S \subseteq \spn(\beta)$ we will have that $\spn(S)\subseteq \spn(\beta)$, and we already know that $\spn(\beta)\subseteq V$. To show this, note that for any $v \in S$ if $v \in \beta$ then trivially $v \in \spn(\beta)$, and if $v \notin \beta$, then by assumption $\beta \cup \{v\}$ is linearly dependent, in which case $v \in \spn(\beta)$ by theorem \ref{thm_span}.
\end{proof}
\begin{thm}
\label{thm_replace}
  Let $V$ be a vector space generated by a set $G$ containing $n$ vectors, and $L \subseteq V$ be linearly independent containing $m$ vectors. Then $m \leq n$
  and there exists a subset $H \subseteq G$ containing $n-m$ vectors such that $\spn(L \cup H)=V$.
\end{thm}
\begin{proof}
  We proceed by induction on $m$. For $m=0$ $L=\emptyset \subseteq V$ and $0 \leq n$ for all $n \in \mathbb{N}$. Taking $H=G$ we are done. So suppose our theorem is true for any linearly independent set with $m-1$ vectors. Now consider an arbitrary linearly independent subset of $V$, $L=\{v_{1}, v_{2}, \dots v_{m}\}$. The set $\{v_{1}, v_{2}, \dots v_{m-1}\} \subseteq L$ is then linearly independent, and so by our induction hypothesis, $m-1 \leq n$ and there is a subset $\{h_{1}, h_{2}, \cdots h_{n-(m-1)}\}$ of $G$ such that $\spn(\{v_{1}, v_{2}, \dots v_{m-1}\} \cup \{h_{1}, h_{2}, \cdots h_{n-(m-1)}\})=V$. That is
  \[v_{m}=a_{1}v_{1}+ \cdots a_{m-1}v_{m-1}+b_{1}h_{1}+ \cdots b_{n-(m-1)}h_{n-(m-1)}\]
  for $a_{i}, b_{i} \in \mathbb{F}$. And $n-(m-1)\neq0$, for otherwise $L$ would not be linearly independent by theorem \ref{thm_span}. This means that $n-(m-1)>0$, or, $n>(m-1)$, from which it follows that $m \leq n$. Moreover, there exists some $b_{i}\neq 0$ as otherwise we would, once again, contradict the linear independence of $L$. Without loss of generality, we have
  \[h_{1}=\frac{v_{m}-(a_{1}v_{1}+a_{2}v_{2}+\cdots a_{m-1}v_{m-1}+b_{2}h_{2}+\cdots b_{n-(m-1)}h_{n-(m-1)})}{b_{1}}.\]
  It follows that $h_{1} \in \spn(L \cup \{h_{2}, \dots, h_{n-(m-1)}\})$,  in which case,
  \[\{v_{1}, \dots v_{m}, h_{1}, \dots h_{n-(m-1)}\} \subseteq \spn(L \cup \{h_{2}, \dots h_{n-(m-1)}\}).\]
  But by our induction hypothesis, $\spn(\{v_{1}, \dots v_{m}, h_{1}, \dots h_{n-(m-1)}\})=V$, and hence,\[\spn(L \cup \{h_{2}, \dots h_{n-(m-1)}\})=V.\]
  since $\{h_{2}, \dots h_{n-(m-1)}\}$ is a subset of $G$ that contains $n-(m-1)-1=n-m$ vectors, we have demonstrated the theorem for $L$ with $m$ vectors.
\end{proof}
\begin{cor}
\label{thm_dim}
If a vector space $V$ is generated by a finite basis then any basis for $V$ is finite and of equal cardinality.
\end{cor}
\begin{proof}
Let $\beta$ and $\gamma$ be bases for $V$ with $m$ and $n$ vectors respectively. We have that $m \leq n$ and $n \leq m$ by theorem \ref{thm_replace}.
\end{proof}
Thus we may safely define the dimension of a vector space:
\begin{defn}
  \label{defn_dim}
The dimension of a vectors space $V$, denoted $\dim(V)$, is the unique cardinality of any basis for $V$.
\end{defn}
\begin{cor}
\label{cor_base}
Suppose that $V$ is a vector space with dimension $n$. Then any linearly independent subset of $V$ containing $n$ vectors is a basis for $V$.
And any generating set for $V$ contains at least $n$ vectors.
Additionally, any linearly independent subset of $V$ can have at most $n$ vectors.
\end{cor}
\begin{cor}
  \label{cor_bassub}
Let $W\subseteq V$ be a subspace. Then $\dim(W)\leq \dim(V)$, and if $\dim(W)=\dim(V)$ then $V=W$.
\end{cor}
\section{Direct Sum}
yup
\section{Tensor Product}
yes
\chapter{Linear Functions}
\section{Linearity}
\section{Matrices}
\section{Abstract Spaces and Isomorphism}
\chapter{Linear Systems of Equations}
\section{Rank}
\begin{defn}
\label{defn_elmop}
  An elementary row or column operation on an $m \times n$ matrix $A$ is defined as one of the following:
  \begin{enumerate}
    \item Interchanging any two rows or columns of $A$
    \item Scaling each entry in a row or or column of $A$
    \item Adding a multiple of one row or column to another row or column of $A$
  \end{enumerate}
  An elementary matrix is the result of applying one of the above to the $n \times n$ identity matrix.
\end{defn}
\begin{thm}
\label{thm_elmop}
  Suppose that $B$ is the result of applying an elementary row operation to $A$. Then there exists an elementary matrix $E$ such that $B=EA$. Furthermore, $E$ is the matrix obtained by performing the same elementary row operation to $I_{n}$ as was performed to convert $A$ into $B$. Similarly, if $B$ is the result of applying
  an elementary column operation to $A$, then there exits an elementary matrix $E$ such that $B=AE$, and $E$ is the result of applying the same elementary column operation to $I_{m}$ as was applied to $A$.
\end{thm}
The proof is a tedious verification of cases; the elementary matrices are defined precisely for this to work.
\begin{defn}
\label{defn_rnk}
The rank of a matrix $A$ is defined as the rank of the linear function $L_{A}=Ax$
\end{defn}
\begin{thm}
\label{rnkeq}
  Let $T: V \to W$ be linear and $A=[T]_{\beta}^{\gamma}$. Then $\rank(T)=\rank(L_{A})$
\end{thm}
\begin{proof}
  Consider the map $\phi_{\beta}:V \to \mathbb{F}^{n}$. That is, the function mapping a vector to its representation in coordinates. This is linear by definition and invertible as we know that any basis represents a vector uniquely as a linear combination of its elements. We have
  \[L_{A}(\mathbb{F}^{n})=L_{A}\phi_{\beta}(V)=\phi_{\gamma}(T(V)).\] It follows that \[\dim(\im(L_{A}))=\dim(\im(T))\] because $\phi_{\gamma}$ is an isomorphism.
\end{proof}
\begin{thm}
\label{thm_rnkprp}
  Let $A$ be an $m \times n$. Let $P$ and $Q$ be invertible $m \times m$ and $n \times n$ matrices, respectively.
  Then
  \begin{enumerate}
    \item $\rank(AQ)=\rank(A)$
    \item $\rank(PA)=\rank(A)$
    \item $\rank(PAQ)$
  \end{enumerate}
  \begin{proof}
    \begin{align}
      \im(L_{AQ})&=\im(L_{A}L_{Q}) \\
                 &= L_{A}L_{Q}(\mathbb{F}^{n}) \\
                 &= L_{A}(L_{Q}((\mathbb{F}^{n})) \\
                 &= L_{A}(\mathbb{F}^{n}) \\
                 &=\im(L_{A})
    \end{align}
    Thus, $\rank(L_{AQ})=\rank(L_{A})$.
    Similarly, $\im(L_{P}L_{A})=L_{P}(\im(L_{A}))=\im(L_{A})$
    and so $\dim(\im(L_{P}L_{A}))=\dim(\im(L_{A}))$ since $P$ is an isomorphism.
    It follows, by applying the previous two results that
    $\rank(PAQ)=\rank(A)$.
  \end{proof}
\end{thm}
\begin{thm}
\label{thm_colm}
Let \[A=\begin{pmatrix}
          a_{11} & \cdots & a_{1n} \\
          \vdots & \ddots & \vdots \\
          a_{1m} & \cdots & a_{mn}

        \end{pmatrix}.\] Then $\rank(A)=\dim \left(
        \spn{ \left \{
          \begin{pmatrix} a_{11} \\ \vdots \\ a_{1m}\end{pmatrix}, \cdots \begin{pmatrix} a_{1n}\\ \vdots \\ a_{mn} \end{pmatrix}
        \right \}
        }\right )
        $
\end{thm}
\begin{proof}
  \begin{align}
    \im(L_{A}) &= L_{A}(\mathbb{F}^{n}) \\
               &= L_{A}(\spn{\{e_{1}, \dots e_{n}\}}) \\
               &= \spn{\{Ae_{1}, \dots, Ae_{n}\}} \\
    &=
        \spn{ \left \{
          \begin{pmatrix} a_{11} \\ \vdots \\ a_{1m}\end{pmatrix}, \cdots \begin{pmatrix} a_{1n}\\ \vdots \\ a_{mn} \end{pmatrix}
        \right \}
        }
  \end{align}
  Furthermore, $\dim(\spn{(X)})$ is nothing but the number of linearly independent vectors in $X$ for any set of vectors $X$. Thus we have shown
  that the rank of a matrix is nothing but the number of linearly independent vectors in its columns.
\end{proof}
\begin{thm}
\label{thm_rrefrnk}
Let $A$ be an $m \times n$ matrix. Then a finite composition of elementary row and column operations applied to $A$ results in a matrix
of the form
\[ \begin{pmatrix}
     I_{\rank(A)} & O_{1} \\
     O_{2} & O_{3}
   \end{pmatrix}
 \]
 where $O_{1}, O_{2}, O_{3}$ are zero matrices.
\end{thm}
\begin{proof}
  First, note that if $A$ is a zero matrix, then by theorem \ref{thm_colm} $\rank(A)=0$, and so $A=I_{0}$, the degenerate case of our claim.
  Suppose otherwise.
  We proceed by induction on $m$, the number of rows of $A$. In the case that $m=1$, we may convert $A$ to a matrix of the form
  \[\begin{pmatrix}
      1 & 0 & \cdots & 0
    \end{pmatrix}\] by first making the leftmost entry $1$ and adding the corresponding additive inverses of the others to the other columns.
  Clearly the rank of the above matrix is $1$ and is of the form
  \[\begin{pmatrix}
I_{1} & O
    \end{pmatrix}\]
  This is another degenerate case, as it lacks zeros below the identity.
  Now suppose that our theorem holds when $A$ has $m-1$ rows.

  To demonstrate that our theorem holds when $A$ is an $m \times n$ matrix, notice that when $n=1$, we can argue that our theorem holds as before, but
  using row operations instead of column operations. This is another degenerate case. For $n>0$, note that there exists an entry $A_{ij}\neq0$
  and by applying at most an elementary row and column operation, we can move $A_{ij}$ to position $1,1$. Additionally, we may transform $A_{ij}$ to
  value $1$, and as before, transform all of the entries in row and column 1 besides $A_{ij}$ to $0$. Thus we have a matrix of the form
  \[\begin{pmatrix}
      1 & 0 & \cdots & 0 \\
      0 & x_{11} & \cdots & x_{1 \ n-1}       \\
      \vdots & \vdots & \ddots &  \vdots  \\
      0 & x_{m-1 \ 1}& \cdots & x_{m-1 \ n-1}
    \end{pmatrix}\]
\end{proof}
The submatrix defined by $x_{ij}$ is of dimension $m-1 \times n-1$ and so must have rank $\rank(A)-1$ as elementary opertations preserve rank and deleting a row and column of a matrix reduces its rank by 1. Furthermore, by our induction hypothesis the above matrix may be converted via a finite number
of elementary operations to a matrix of the form
\[\begin{pmatrix}
    1 & 0 & \cdots & 0 \\
    0 & I_{\rank(A)-1} & O_{1} \\
    \vdots &  &   \\
    0 & O_{2} & O_{3}
  \end{pmatrix}\]
Therefore, for an $m \times n$ matrix $A$, a finite number of elementary operations converts it into a matrix of the form
\[\begin{pmatrix}
    I_{\rank(A)} & O_{1} \\
    O_{2} & O_{3} \\
  \end{pmatrix}\].
\begin{thm}
\label{thm_colrow}
  For any matrix $A$, $\rank(A^{T})=\rank(A)$.
\end{thm}
\begin{proof}
  By theorem \ref{thm_rrefrnk}, we may convert $A$ to a matrix $D=BAC$ where $B=E_{1}\cdots E_{p}$ and $C=G_{1}\cdots G_{q}$ where $E_{i}$ and $G_{i}$ are elementary row and column matrices respectively. It follows that $D^{T}=C^{T}A^{T}B^{T}$, whence $\rank(A^{T})=\rank(D^{T})$ by theorem (insert) because elementary
  matrices are invertible, and so is the transpose of the compositions thereof. Further, $D^{T}$ must be of the same form as $D$ since the only nonzero entries of $D$ are along the diagonal from entry $1,1$ to entry $\rank(A), \rank(A)$. Hence, we have $\rank(A)$ linearly independent columns in the matrix $D^{T}$.

  Since the columns of $D^{T}$ are the rows of $D$, we see that the number of linearly independent columns of $A$ is equal to the number of linearly
  independent columns of $A^{T}$. In other words, the dimension of the space generated  by the columns of $A$ is equal to the dimension of the space generated by its rows.
\end{proof}
\begin{thm}
\label{thm_invm}
  Let $A$ be an invertible $n \times n$ matrix. Then $A$ is a product of elementary matrices.
\end{thm}
\begin{proof}
  By the dimension theorem, if $A$ is invertible,  then $\rank(A)=n$. So by theorem \ref{thm_rrefrnk} $A$ may converted into a matrix of the form
  $I_{n}=E_{1}\cdots E_{p}AG_{1} \cdots G_{q}$, whence $A=E_{1}^{-1}\cdots E_{p}^{-1}I_{n}G_{1}^{-1}\cdots G_{q}^{-1}$.
\end{proof}
\begin{thm}
\label{thm_rnkinq}
  Let $T:V \to W$ and $U:W \to Z$.  Then
  \begin{enumerate}
    \item $\rank(TU)\leq \rank(U)$
    \item $\rank(TU) \leq \rank(T)$
  \end{enumerate}
\end{thm}
\begin{proof}
  We have
  \begin{align}
    \rank(TU)&=\dim(\im(TU)) \\
            &= \dim(\im(T(U(V)))) \\
            &\subseteq U(W) \\
            &= \im(U)
  \end{align}
  Therefore, $\dim(\im(TU))\leq \dim(\im(U))$. Next, let $\beta, \gamma, \phi$ be ordered bases for $V, W, $ and $Z$, respectively; and let
  $A=[T]_{\beta}^{\gamma}$ and $B=[U]_{\gamma}^{\phi}$.
  By theorem \ref{thm_colrow}
  \begin{align}
    \dim(\im(TU))&=\dim(\im(AB)) \\
                 &= \dim(\im((AB)^{T}) \\
                 &= \dim(\im(B^{T}A^{T})) \\
                 &\leq \dim(\im(A^{T})) \\
                 &= \dim(\im(A)) \\
    &= \dim(\im(T))
  \end{align}
\end{proof}
\section{Form}
We now apply the fruits of our investigation into vector spaces and linearity to solve systems of linear equations.
\begin{defn}
\label{def_linsys}
  A linear system of equations is a collection of $m$ equations of the form:
  \[ a_{1}x_{1}+ \cdots +a_{n}x_{n}=b \] where $a_{i}, x_{i},b \in \mathbb{F}$ for $1 \leq i \leq n$.
  Equivalently, we may say
  $Ax=b$ for an $m \times n$ matrix $A$, where $x= \begin{pmatrix} x_{1} \\ \vdots \\ x_{n}\end{pmatrix}$ and $b= \begin{pmatrix} b_{1} \\ \vdots \\ b_{m}\end{pmatrix}$.
  If $b=\bold{0}$, the linear system is said to be homogenous.
\end{defn}
\begin{defn}
\label{def_soln}
A solution to a linear system is a vector $s \in \mathbb{F}^{n}$ such that $As=b$
\end{defn}

\begin{thm}
\label{thm_homsoln}
Let $A$ be an $m \times n$ matrix over $\mathbb{F}$. If $m<n$, then the homogenous system $Ax=0$ has a nontrivial solution.
\end{thm}
\begin{proof}
  Notice that, the solution set to the system $Ax=0$ is $\ker(L_{A})$, so by the dimension theorem, $\dim(\ker(A))=n-\rank(L_{A})$. Additionally,
  we know that $\rank(A)$ is nothing but the number of linearly independent vectors defined by its rows which certainly cannot exceed $m$. Therefore
  $\rank(A) \leq m < n$, in which case $n-\rank(A)=\dim(\ker(A))>0$, and so $\ker(A)\neq \{0 \}$.
\end{proof}
\begin{thm}
\label{thm_gensoln}
For any solution $s$ to the linear system $Ax=b$, \[\{s+s_{0}:As_{0}=\bold{0} \}\] is its solution set.
\end{thm}
\begin{proof}
Suppose that $As=b$ and $As'=b$. Then $A(s'-s)=As'-As=b-b=0$. It follows that $s+(s'-s) \in S$. Conversely, if $y \in S$, then $y=s+s'$, in which case $Ay=A(s+s')=As+As'=b+0=b$. That is, $Ay=b$.
\end{proof}
\begin{thm}
\label{thm_uniqsoln}
  Let $Ax=b$ for an $n \times n$ matrix $A$. If $A$ is invertible, then the system has a single solution $A^{-1}b$. If the system has a single solution, then $A$ is invertible.
\end{thm}
\begin{proof}
Suppose $A$ is invertible. Then $A(A^{-1}b)=AA^{-1}(b)=b$. Furthermore, if $As=b$ for some $s \in \mathbb{F}^{n}$, then $A^{-1}(As)=A^{-1}b$ and so $s=A^{-1}b$.
Next, suppose that the system has a unique solution $s$. Then by theorem \ref{thm_gensoln}, we know that the solution set $S=\{s+s_{0}: As_{0}=0 \}$. But this is only the case if $\ker(A)=\{0\}$, lest $s$ not be unique. And so, by the dimension theorem, $A$ is invertible.
\end{proof}
\begin{thm}
\label{thm_nonemsoln}
The linear system $Ax=b$ has a nonempty solution set if and only if $\rank(A)=\rank(A|b)$.
\end{thm}
\begin{proof}
  If the system has a solution, then $b \in \im(L_{A})$. Additionally, $\im(L_{A})=L_{A}(F^{n})$ and $L_{A}(e_{i})=Ae_{i}=
  \begin{pmatrix} a_{1i} \\ \vdots \\ a_{ni}   \end{pmatrix}$. Therefore, since $L_{A}(\mathbb{F}^{n})= \spn \{Ae_{1}, \dots Ae_{n}\}$,
  $\im(L_{A})=\spn\{A_{1}, \dots A_{n}\}$, where $A_{i}$ is the $i^{th}$ column of $A$.
  Certainly, $b \in \spn\{A_{1}, \dots A_{n}\}$ if and only if $\spn\{A_{1}, \dots A_{n}\}=\spn\{A_{1}, \dots A_{n}, b\}$, which is to say
  $\dim(\im(\spn\{A_{1}, \dots A_{n}\}))=\dim(\im(\spn\{A_{1}, \dots A_{n}, b\}))$, or, $\rank(A)=\rank(A|b)$.
\end{proof}
\begin{cor}
\label{cor_infsoln}
Let $Ax=b$ be a linear system of $m$ equations in $n$ variables. Then its solution set is either, empty, of one element, or of infinitely many elements (provided that $\mathbb{F}$ is not a finite field).
\end{cor}
\begin{proof}
  By theorem \ref{thm_nonemsoln} $Ax=b$ has a nonempty solution set if and only if $\rank(A)=\rank(A|b)$. Therefore, it may be that our linear system has no solutions; however, supposing that this is not the case, by theorem \ref{thm_uniqsoln} it has a unique solution if and only if $A$ is invertible. Finally, assume that our linear system has neither no solution nor a single solution. This yields
  \begin{equation}
Ax_{1}=Ax_{2}=b
\end{equation}
for $x_{1}, x_{2} \in \mathbb{F}^{n}$, which implies
\begin{align}
  Ax_{1}-Ax_{2}&=\bold{0} \\
               &=A(x_{1}-x_{2}) \\
               &= nA(x_{1}-x_{2}) \\
               &= A(n(x_{1}-x_{2})) \\
\end{align}
where $n \in \mathbb{F}$.
Thus, by theorem \ref{thm_gensoln}
\[A(x_{1}+n(x_{1}-x_{2}))=b.\]
\end{proof}
\section{Solution}
\begin{defn}
\label{defn_rref}
A matrix of the form \[\begin{pmatrix} a_{11} & \cdots & a_{1n}\\ \vdots & \ddots & \vdots \\a_{m1} & \cdots & a_{mn}\end{pmatrix}\] is said to be in reduced echelon form if
\begin{enumerate}
  \item $a_{ii}\neq 0$ implies that $a_{ij}=1$
  \item $a_{ij} \neq 1$ implies that $a_{ij}=0$
  \item $a_{ij}=0$ for all $1\leq j \leq n$ implies that $i < r$ for all nonzero rows $\begin{pmatrix}a_{r1} & \cdots & a_{rn}\end{pmatrix}$
\end{enumerate}
\end{defn}
\begin{thm}
\label{thm_rref}
Any matrix can be converted into reduced echelon form via a finite number of elementary row operations.
\end{thm}
\begin{proof}
This is a restatement of theorem \ref{thm_rrefrnk}.
\end{proof}
This form is of particular interest because reducing an augmented matrix is equivalent to solving a linear system of equations. We now have a
procedure for solving arbitrary systems of linear equations. For example, we may now demonstrate that a set of vectors is linearly dependent by
finding a nontrivial solution to a linear system of equations; similarly we may apply theorem \ref{thm_nonemsoln} to demonstrate that a set of vectors is linearly dependent. In the following chapter, we will also see that computing the elements of an eigenspace is made possible by reducing a matrix.
It follows that
\begin{cor}
\label{cor_soln}
For any invertible $n \times n$ matrix $A$.
\[A^{-{1}}(A|I_{n}) = E_{1} \cdots E_{p}(A|I_{n})=(I_{n}|A^{-1})\]
where $E_{1}, \dots, E_{p}$ are elementary matrices.
\end{cor}
Notice that the above elementary matrices may be either row or column matrices; however, since we are left multiplying, the product will result in a
row operation.
Thus we now have a procedure for finding the inverse of any matrix: perform row operations to convert it into the identity matrix, while accounting for each change.
Additionally,
\begin{cor}
\label{cor_solninvar}
Let $A$ be an $m \times n$ matrix and $C$ be an invertible $n \times n$ matrix. Then the solutions sets to the linear systems
\[Ax=b \text{and} CAx=Cb \] are equal.
\end{cor}
This follow directly from the invertibility, and fits with our intuition: as we row reduce a linear system, its solutions do not change.
\chapter{The Determinant}
\section{Permuations}
define determinant
show equal to cofactor expansion
\section{Cofactor Expansion}
deduce enough properties to define the determinat more formally
\section{Multilinear and Alternating}
demonstrate cofactor expansion is unqiue multilinear alternating etc
hence permutation=cofactor=unique such function
\section{Properties}
deduce remaining important properties
need invertible iff det nonzero
\section{Measure}
\chapter{Eigenspaces}
\section{Characteristic Polynomial}
\section{Diagonalization and Similarity}
\chapter{Orthogonality}
\section{Inner Products}
Hello
\section{Projections}
\begin{defn}
\label{defn_prj}
Let $V=W_{1}\oplus W_{2}$. A projection of $V$ on $W_{1}$ along $W_{2}$ is a linear function $T:V \to V$ such that for any $x \in V$ where $x=x_{1}+x_{2}$ $x_{1} \in W_{1}$ and $x_{2} \in W_{2}$ $T(x)=x_{1}$.
\end{defn}
\begin{thm}
\label{thm_projiff}
A linear function $T:V \to V$ is a projection of $V$ on $W_{1}=\{x : T(x)=x \}$ along $\ker{T}$ if and only if $T=T^{2}$.
\end{thm}
\begin{proof}
  If $T$ is a projection, then clearly $T=T^{2}$ by definition. Conversely, for $x \in V$ we know that $x=Tx+(x-Tx)$. But by assumption
  $T^{2}x=Tx$, which means $T(Tx-x)=T(x-Tx)\bold{0}$. That is, $x-Tx \in \ker(T)$. Hence, $V=\{x \in V: Tx=x\} \oplus \ker(T)$ as $Tx=x$ and $Tx=0$  implies $x=0$($x \in \ker(T)$). And so for $x \in V$, we have $x=y+z$ for $y \in \{x \in V: Tx=x\}$ and $z\in \ker(T)$, and so $Tx=Ty+Tz=y$.
\end{proof}
\section{Orthogonal Projection}
\begin{defn}
\label{orthcomp}
Let $W \subseteq V$. The orthogonal complement of $W$ is defined as $W^{\perp}=\{v \in V : \langle v, w \rangle=0 \text{ for all } w \in W\}$.
\end{defn}
\begin{thm}
The following statments are true
\begin{enumerate}
  \item $W^{\perp}$ is a subspace of $V$
  \item $\dim(W^{\perp})=\dim(V)-\dim(W)$
\end{enumerate}
\end{thm}
\begin{proof}
  Firstly, note that $ \langle \bold{0}, w\rangle=\bold{0}$ for all $w \in W$, so $\bold{0} \in W^{\perp}$. Furthermore, if $\langle w, c\rangle=0$ for some $w \in W$ then
  $\langle aw, c \rangle = a \langle w, c \rangle =0 $ by linearity. Similarly, if $\langle w, a \rangle=0  $ and $\langle b, c \rangle=0  $ then $\langle w, a \rangle+ \langle b, c \rangle= \langle w+b, c \rangle =0$. Secondly,
\end{proof}
\begin{thm}
\label{thm_orthodecomp}
  Let $W \subseteq V$.Then for any $x \in V$ there exist unique vectors $y \in W$ and $z \in W^{\perp}$ such that $x=y+z$. Furthermore, for all $w \in W$
s  \[ ||y-x|| \leq ||w-x||\] and we call $y$ the orthogonal projection of $z$ on $w$, denoted $x_{w}$. Similarly, $z$ is denoted $x_{\perp}$.
\end{thm}
\begin{proof}
trivial
\end{proof}

\begin{thm}
Let $W \subseteq V$ $x \in V$ and $\beta=\{v_{1}, \dots v_{n}\}$ be an orthonormal basis for $W$ and $A$ be the matrix whose $j^{th}$ column is $v_{j}$. Then the orthogonal projection of $x$ on $W$ $x_{w}=AA^{*}x$.
\end{thm}

\begin{proof}
  We begin by demonstrating that $W^{\perp}=\ker A^{*}$. We have
  \[A^{*}x= \begin{pmatrix}v_{1}^{*}x \\ \vdots \\ v_{n}^{*}x \end{pmatrix}=\begin{pmatrix}\langle v_{1}, x \rangle \\  \vdots \\ \langle v_{n}, x \rangle\end{pmatrix}.\] Certainly
  $Ax=\bold{0}$ if and only if $\langle v_{i}, x \rangle=0$ for all $1 \leq i \leq n$. But that is to say $x \in W^{\perp}$, and so \[\ker(A^{*})=W^{\perp}.\]
  Note that $Ax=\spn\beta$ by definition. Therefore, for some $c \in \mathbb{F}^{n}$ $Ac=x_{w}$, which means that $x-x_{W}=x-Ac \in W^{\perp}$. It follows that
  $A^{*}(x-Ac)=0$ and so \[A^{*}Ac=A^{*}x.\] Thus, if we see that $x_{w}=Ac$. Furthermore, since $\beta$ is orthonormal, $A$ must be unitary, in which case \[Ac=AA^{*}x=x_{W}.\]
\end{proof}
\begin{cor}
  \label{cor_orthproj}
$AA^{*}$ is a projection and $\ker(AA^{*})=W^{\perp}$. Additionally, $AA^{*}$ is the unique such linear function.
\end{cor}
\begin{proof}
  Surely $AA^{*}$ is linear, and since we know that $x=x_{W}+x_{W^{\perp}}$ for all $x \in V$ it follows that $(AA^{*})^{2}x=AA^{*}x_{W}=x_{w}=AA^{*}x$. Thus the orthogonal projection is, in fact, a projection on $W^{\perp}=\{x \in V: AA^{*}x=x \}$ along $\ker(AA^{*})$, by theorem \ref{thm_projiff} ($V=W \oplus W^{\perp}$). Furthermore, if $x=x_{W}+x_{W^{\perp}}$ with $x_{W}=0$, $AA^{*}x=x_{W}=0$. The converse follows in the same way. Thus, $\ker(AA^{*})=W^{\perp}$ Similarly, we have $\im(AA^{*})=W$. Additionally, as a projection is defined uniquely in terms of its range, it is clear that any other projection $T$ on $W=\{x \in V: T(x)=x\}$ must be the same as $AA^{*}$.
\end{proof}
\section{The Adjoint}
\section{Normal and Unitary Operators}
self adjoint iff orthogonal projection
all unitary operators are rotations
\section{Definiteness}
\chapter{Matrix Decomposition}
\section{LU etc.}
\section{Schur's Theorem}
\section{Spectral Theorem}
\section{Singular Value Decomposition}
\section{Polar Decomposition}
\appendix
\chapter{Set Theory}
Axiom of choice
\chapter{The Complex Field}
fundamental theorem of algebra
\chapter{Block Matrices}
need to prove result for diagonalization proof
\chapter{Multilinearity and Sesquilinearity}
pos definite matrices generate inner products uniquely
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{References}
\printbibliography[title={References}]





\end{document}
