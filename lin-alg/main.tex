\documentclass[oneside, 12pt]{book}
\usepackage{indentfirst}
\usepackage[backend=biber]{biblatex}
\addbibresource{refs.bib}
\usepackage{amsthm, amsmath, amssymb, amsfonts}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\im}{im}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black,
    bookmarks=true,
  }
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{lem}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\renewcommand\qedsymbol{$\blacksquare$}
\title{Finite Dimensional Inner Product Spaces}
\author{Jason Kenyon}
\date{November 2022}

\begin{document}
\frontmatter
\maketitle
\cleardoublepage
\pdfbookmark{\contentsname}{toc}
\tableofcontents
\chapter*{Preface}
Hello
\addcontentsline{toc}{chapter}{Preface}
\mainmatter
\chapter{Vector Spaces}
\section{Spaces and Subspaces}
\begin{defn}
\label{defn_vspace}
  A vector space $V$ over a field $\mathbb{F}$ is a set, along with a binary operation $+: V^{2} \to V$ and a binary operation $\boldsymbol{\cdot}: \mathbb{F} \times V \to V$ that satisfy the following properties:
  \begin{enumerate}
    \item $a\boldsymbol{\cdot}v+w \in V$
    \item $v+w=w+v$
    \item $v+(w+z)=(v+w)+z$
    \item $\bold{1}v=v$
    \item $(a\boldsymbol{\cdot}b)x=a\boldsymbol{\cdot}(bx)$
    \item $a\boldsymbol{\cdot}(v+w)=av+aw$
    \item $(a+b)v=a\boldsymbol{\cdot}v+b\boldsymbol{\cdot}v$
    \item There exists an element $\bold{0} \in V$ such that $v+\bold{0}=v$
    \item There exists an element $v^{-1}$ such that $v+v^{-1}=\bold{0}$
  \end{enumerate}
  for all $a, b \in \mathbb{F}$ and $v,w,z \in V$.
\end{defn}
The elements of $V$ are called vectors, and the elements of $\mathbb{F}$ are called scalars. The operations $+$ and $\boldsymbol{\cdot}$ are called vector addition and scalar multiplication, respectively. We omit the $\boldsymbol{\cdot}$ and do not explicitly apply $+$ for clarity. The $\bold{1}$ is the identity element of $\mathbb{F}$. $-a$ will denote the additive inverse of $a \in \mathbb{F}$ for the field $\mathbb{F}$, and $-v=v^{-1}$ will denote the additive inverse of a vector $v \in V$ under vector addition, while $-a(v)$ will denote multiplication of a vector by a scalar's additive inverse in $\mathbb{F}$.

\begin{thm}
\label{thm_canc}
  Let $x$, $y$, and $z$ be vectors in $V$. If $x+z=y+z$, then $x=y$.
  The zero element of $V$ is unique.
  The additive inverse in $V$ is unique for each vector in $V$.
\end{thm}
\begin{thm}
\label{thm_alg}

\begin{enumerate}
  \item $0(x)=\bold{0}$
  \item $(-a)x=-(ax)=a(-x)$
  \item $a(\bold{0})=\bold{0}$
\end{enumerate}

\end{thm}
\begin{defn}
\label{defn_subspace}
  A subspace $W$ of a vector space $V$ is a set $W \subseteq V$ that is itself a vector space.
\end{defn}
\begin{thm}
\label{thm_subspace}
  Let $V$ be a vector space with zero element $\bold{0}$. Then a subset $W \subseteq V$
  is a subspace of $V$ if and only if \[\bold{0} \in W\] and \[cx+y \in W\] for all $x,y \in W$ and $c \in \mathbb{F}$.

\end{thm}
\begin{thm}
\label{thm_spn}
Let $S$ be a subset of a vector space $V$. Then $\spn(S)$ is a subspace of $V$, and if any subspace of $V$ contains $S$ must necessarily contain $\spn(S)$
\end{thm}
The proof follows directly from the definition of span. The span is defined precisely to generate a subspace in this way. Additionally, it should be clear that any linear combination of vectors in a subspace must be contained in that subspace as this is the defining characteristic of a vector space.
\section{Linear Independence}
\begin{defn}
\label{defn_linind}
  A set of vectors $\{v_{1}, v_{2}, \dots v_{n}\}$ is linearly dependent if
  \[a_{1}v_{1}+a_{2}v_{2}+ \dots a_{n}v_{n}=\bold{0}\] for $a_{1}, a_{2}, \dots a_{n} \in \mathbb{F}$ not all zero. Similarly, a set of vectors is linearly independent if it is not linearly dependent.
\end{defn}
\begin{thm}
\label{thm_linind}
If $S_{1}\subseteq S_{2} \subseteq V$ and $S_{1}$ is linearly dependent, then $S_{2}$ is linearly dependent as well. Similarly, if $S_{2}$ is linearly dependent, then $S_{1}$ is linearly dependent.
\end{thm}
\begin{proof}
The proof should be clear when considering the above definition.
\end{proof}
\begin{thm}
\label{thm_span}
Let $S$ be a linearly independent subset of $V$ and $v \in V$ such that $v \in S$. Then $S \cup \{v\}$ is linearly dependent if and only if $v \in \spn(S)$.
\end{thm}
\begin{proof}
  If $S \cup \{v\}$ is linearly dependent then there exist scalars $a_{1}, a_{2}, \dots a_{n}, a_{v} \in \mathbb{F}$ not all zero such that \[a_{1}s_{1} +a_{2}s_{2}+\cdots +a_{n}s_{n}+a_{v}v=\bold{0}.\]
  Therefore, $a_{v}\neq0$, for otherwise we would contradict the linear independence of $S$. This implies that \[v=-\frac{a_{1}s_{1}+ a_{2}s_{2}+ \cdots a_{n}s_{n}}{a_{v}}.\] and hence $v \in \spn(S)$. Conversely, if $v \in \spn(S)$, then \[v=a_{1}s_{1}+a_{2}s_{2}+\cdots a_{n}s_{n} \] for some scalars $a_{1},a_{2}, \cdots \in \mathbb{F}$ This implies that \[1(v)-(a_{1}s_{1}+\cdots a_{n}s_{n})=\bold{0}.\] which is a nontrivial solution, so the set $S \cup \{v\}$ is linearly dependent.
\end{proof}
\section{Bases}
\begin{defn}
\label{defn_basis}
A subset $\beta \subseteq V $ is a basis for $V$ if it is a linearly independent set such that $\spn(\beta)=V$.
\end{defn}
\begin{thm}
\label{thm_uniqbas}
A subset $\beta=\{v_{1},v_{2}, \dots v_{n}\}$ of $V$ is a basis for $V$ if and only if for any vector $v \in V$ \[v=a_{1}v_{1}+\cdots a_{n}v_{n}\] for unique scalars $a_{1}, \dots a_{n} \in \mathbb{F}$.
\end{thm}
\begin{proof}
Suppose that $\beta=\{v_{1}, \dots v_{n}\}$ is a linearly independent generating set of $V$. Then $v=a_{1}v_{1}+\cdots a_{n}v_{n}$ for scalars $a_{1}, \dots a_{n} \in \mathbb{F}$. Further, suppose that there exists another collection $b_{1}, \dots b_{n}$ of scalars such that $v=b_{1}v_{1}+\cdots b_{n}v_{n}$. Subtracting, we have \[(a_{1}-b_{1})v_{1}\cdots (a_{n}-b_{n})v_{n}=\bold{0}.\] Since $\beta$ is linearly independent, it follows that $a_{i}-b_{i}=0$, and hence $a_{i}=b_{i}$ for all $1 \leq i\leq n$. Therefore, the linear combination $a_{1}v_{1}+\cdots a_{n}v_{n}$ is the unique representation of $V$ for $\beta$. Similarly, if we know that $v=a_{1}v_{1}\cdots a_{n}v_{n}$ for unique scalars, then \[(b_{1})v_{1}+\cdots(b_{n})v_{n}=\bold{0}=v-v.\] if and only if $b_{i}=a_{i}-a_{i}=0$ for all $1 \leq i \leq n$. And certainly $V=\spn(\beta)$, so $\beta$ is a basis for $V$.
\end{proof}




see this source \textcite{jechset}.
\begin{thm}
\label{thm_basis}
Every vector space has a basis.
\end{thm}
\begin{proof}
  Consider the set $L$ of all linearly independent subsets of a vector space $V$. Let $T \subseteq L$ be a chain. That is, for any two sets $A$ and $B$ in $T$ either $A \subseteq B$ or $B \subseteq A$. Hence, any finite subset of $\bigcup T$ is in $L$. In other words, taking a union over a chain yields an upper bound under $\subseteq$ which must necessarily be in the set from whence it came. This ensures that $T$ is linearly ordered by $\subseteq$, for transitivity, reflexivity, and antisymmetry are already satisfied by definition of a subset. Therefore, Zorn's lemma implies that there exists a maximal element in $L$. That is, there exists an element $l \in L$ such that for all $A \in L$ $A \subseteq l$. Moreover, we know that $l$ is linearly independent by assumption.

  To show that $l$ spans $V$, suppose that there were an element $v \in V$ such that $v\notin \spn(l)$. Then by theorem \ref{thm_span} $l \cup \{v\}$ would be a linearly independent set, in which case $l \cup \{v\} \in L $. But $l \cup \{v \} \nsubseteq l$, contradicting the fact that $l$ is the maximal element of $L$.
\end{proof}
\begin{cor}
\label{cor_bascard}
If $V$ is generated by a finite set, then there exists a finite basis for $V$ contained within the generating set.
\end{cor}
\begin{proof}
  Suppose that $\spn(S)=V$ for a finite set $S$. Consider an arbitrary linearly independent subset $\beta \subseteq S$ such that $\beta \cup \{v\}$ is linearly dependent for any $v \in S$ such that $v \notin \beta$. Such a set certainly exist because any set containing a single vector is linearly independent, and so we may continue to add vectors from $S$ into $\beta$ until another union results in a linearly dependent set. Hence if we demonstrate that $S \subseteq \spn(\beta)$ we will have that $\spn(S)\subseteq \spn(\beta)$, and we already know that $\spn(\beta)\subseteq V$. To show this, note that for any $v \in S$ if $v \in \beta$ then trivially $v \in \spn(\beta)$, and if $v \notin \beta$, then by assumption $\beta \cup \{v\}$ is linearly dependent, in which case $v \in \spn(\beta)$ by theorem \ref{thm_span}.
\end{proof}
\begin{thm}
\label{thm_replace}
  Let $V$ be a vector space generated by a set $G$ containing $n$ vectors, and $L \subseteq V$ be linearly independent containing $m$ vectors. Then $m \leq n$
  and there exists a subset $H \subseteq G$ containing $n-m$ vectors such that $\spn(L \cup H)=V$.
\end{thm}
\begin{proof}
  We proceed by induction on $m$. For $m=0$ $L=\emptyset \subseteq V$ and $0 \leq n$ for all $n \in \mathbb{N}$. Taking $H=G$ we are done. So suppose our theorem is true for any linearly independent set with $m-1$ vectors. Now consider an arbitrary linearly independent subset of $V$, $L=\{v_{1}, v_{2}, \dots v_{m}\}$. The set $\{v_{1}, v_{2}, \dots v_{m-1}\} \subseteq L$ is then linearly independent, and so by our induction hypothesis, $m-1 \leq n$ and there is a subset $\{h_{1}, h_{2}, \cdots h_{n-(m-1)}\}$ of $G$ such that $\spn(\{v_{1}, v_{2}, \dots v_{m-1}\} \cup \{h_{1}, h_{2}, \cdots h_{n-(m-1)}\})=V$. That is
  \[v_{m}=a_{1}v_{1}+ \cdots a_{m-1}v_{m-1}+b_{1}h_{1}+ \cdots b_{n-(m-1)}h_{n-(m-1)}\]
  for $a_{i}, b_{i} \in \mathbb{F}$. And $n-(m-1)\neq0$, for otherwise $L$ would not be linearly independent by theorem \ref{thm_span}. This means that $n-(m-1)>0$, or, $n>(m-1)$, from which it follows that $m \leq n$. Moreover, there exists some $b_{i}\neq 0$ as otherwise we would, once again, contradict the linear independence of $L$. Without loss of generality, we have
  \[h_{1}=\frac{v_{m}-(a_{1}v_{1}+a_{2}v_{2}+\cdots a_{m-1}v_{m-1}+b_{2}h_{2}+\cdots b_{n-(m-1)}h_{n-(m-1)})}{b_{1}}.\]
  It follows that $h_{1} \in \spn(L \cup \{h_{2}, \dots, h_{n-(m-1)}\})$,  in which case,
  \[\{v_{1}, \dots v_{m}, h_{1}, \dots h_{n-(m-1)}\} \subseteq \spn(L \cup \{h_{2}, \dots h_{n-(m-1)}\}).\]
  But by our induction hypothesis, $\spn(\{v_{1}, \dots v_{m}, h_{1}, \dots h_{n-(m-1)}\})=V$, and hence,\[\spn(L \cup \{h_{2}, \dots h_{n-(m-1)}\})=V.\]
  since $\{h_{2}, \dots h_{n-(m-1)}\}$ is a subset of $G$ that contains $n-(m-1)-1=n-m$ vectors, we have demonstrated the theorem for $L$ with $m$ vectors.
\end{proof}
\begin{cor}
\label{thm_dim}
If a vector space $V$ is generated by a finite basis then any basis for $V$ is finite and of equal cardinality.
\end{cor}
\begin{proof}
Let $\beta$ and $\gamma$ be bases for $V$ with $m$ and $n$ vectors respectively. We have that $m \leq n$ and $n \leq m$ by theorem \ref{thm_replace}.
\end{proof}
Thus we may safely define the dimension of a vector space:
\begin{defn}
\label{defn_dim}
The dimension of a vectors space $V$, denoted $\dim(V)$, is the unique cardinality of any basis for $V$.
\end{defn}
\begin{cor}
\label{cor_base}
Suppose that $V$ is a vector space with dimension $n$. Then any linearly independent subset of $V$ containing $n$ vectors is a basis for $V$.
And any generating set for $V$ contains at least $n$ vectors.
Additionally, any linearly independent subset of $V$ can have at most $n$ vectors.
\end{cor}
\begin{cor}
\label{cor_bassub}
Let $W\subseteq V$ be a subspace. Then $\dim(W)\leq \dim(V)$, and if $\dim(W)=\dim(V)$ then $V=W$.
\end{cor}
\section{Direct Sum and Projections}
\begin{defn}
  \label{defn_dirsum}
Let $V$ be a vector space with subspaces $W_{1}$ and $W_{2}$ such that $W_{1}\cap W_{2}=\{\bold{0}\}$. Then we call the direct sum of $W_{1}$ and $W_{2}$ \[W_{1}\oplus W_{2}= \{w_{1}+w_{2}: w_{1} \in W_{1} \text{ and } w_{2} \in W_{2}\}.\]
\end{defn}
\begin{thm}
  \label{thm_dirsum}
  Let $V=W_{1} \oplus W_{2}$ for some subspaces $W_{1}, W_{2}$ of $V$. If $\{v_{1}, \dots v_{k}\}$ is a basis for $W_{1}$ and $\{w_{1}, \dots w_{l}\}$ is  a basis for $W_{2}$ then $\{v_{1}, \dots v_{k}, w_{1}, \dots w_{l}\}$ is a basis for $V$. Hence $\dim(V)=\dim(W_{1})+\dim(W_{2})$.
\end{thm}
\begin{proof}
  Let $x \in V$ and $x=x_{1}+x_{2}$ for $x_{1} \in W_{1}$ and $x_{2} \in W_{2}$. Then \[x=a_{1}v_{1}+ \cdots a_{k}v_{k}+b_{1}w_{1}+ \cdots b_{l}w_{l}\]
  If $x=\bold{0}$ then it must be that $x_{1}=-x_{2}$. Hence $x_{1}=x_{2}=\bold{0}$ for otherwise $-x_{2} \in W_{1}$ and $-x_{2}\notin W_{2}$, which would contradict our assumption that $W_{2}$ is a subspace of $V$. Moreover, since $\{v_{1}, \dots v_{k}\}$ and $\{w_{1}, \dots w_{l}\}$ are both linearly indpendent, it must be that $a_{i}=0$ for all $1\le i \leq k$ and $b_{i}=0$ for all $1 \leq i \leq l $.
\end{proof}
\begin{defn}
\label{defn_prj}
 If $V=W_{1}\oplus W_{2}$ then a projection of $V$ on $W_{1}$ along $W_{2}$ is a linear function $T:V \to V$ such that for any $x \in V$ where $x=x_{1}+x_{2}$ $x_{1} \in W_{1}$ and $x_{2} \in W_{2}$ $T(x)=x_{1}$.
\end{defn}
\begin{thm}
\label{thm_projiff}
A linear function $T:V \to V$ is a projection of $V$ on $W_{1}=\{x : T(x)=x \}$ along $\ker{T}$ if and only if $T=T^{2}$.
\end{thm}
\begin{proof}
  We have, for all $x \in V$ $x=x_{1}+x_{2}$ where $x_{1} \in W_{1}$ and $x_{2} \in \ker(T)$ so that
  \begin{align}
    TT(x)&=TT(x_{1}+x_{2}) \\
         &=Tx_{1} \\
         &= x_{1} \\
    &= Tx
  \end{align}
  Conversely, if $T^{2}=T$, we know that for all $x \in V$ $Tx=Tx+(x-Tx)$. This implies that $T(Tx)=Tx+\bold{0}$. Since $T(Tx)=Tx$, $Tx \in \{y: Ty=y\}$. Furthermore, If $Tx=x$ and $Tx=\bold{0}$ for some $x \in V$ then $x= \bold{0}$. That is \[\{y:Ty=y\}\cap \ker T=\{\bold{0}\}\] hence
  $V= \{y: Ty=y\} \oplus \ker T$ and so for any $x \in V$ we have $x=x_{1}+x_{2}$ for some $x_{1}\in \{y:Ty=y\}$ and $x_{2} \in \ker(T)$
  whence
  \[Tx=x_{1}\]
\end{proof}

\chapter{Linear Functions}
\section{Linearity}
\begin{defn}
\label{defn_lin}
  A function $f: V \to W$ between two vector spaces $V$ and $W$ is linear if
  \[f(ax+y)=af(x)+f(y)\] for all $x, y \in V$ and $a \in \mathbb{F}$.
\end{defn}
The following properties of linear functions go without saying:
\begin{enumerate}
  \item $f(\bold{0})=\bold{0}$
  \item $f(\sum_{i=1}^{n}a_{i}x_{i})=\sum_{i=1}^{n}a_{i}f(x_{i})$
\end{enumerate}
It follows that linear functions are unique up to how they map basis elements.
\begin{cor}
\label{cor_linbas}
  Let $f:V \to W$ and $g: V \to W$ be linear and $\{v_{1}, \dots, v_{n}\}$ be a basis for $V$. Then $f=g$ if and only if $f(v_{i})=g(v_{i})$.
\end{cor}

\begin{defn}
\label{defn_kerim}
For a linear function $f: V \to W$ we define \[ \im(f)=\{y: f(x)=y \text{ for some } x\in V \} \]
and
\[\ker(f)=\{x \in V: f(x)=\bold{0} \}.\]
\end{defn}
\begin{thm}
\label{thm_kerim}
Let $f: V \to W$ be a linear function. Then $\ker(f)$ and $\im(f)$ are subspaces of $V$ and $W$ respectively.
\end{thm}
\begin{proof}
  We begin with $\ker(f)$. Surely, $\ker(f) \subseteq V$, so suppose that $x,y \in \ker(f)$ and $a \in \mathbb{F}$. We have \[f(x)=f(y)=\bold{0}\] hence \[af(x)+f(y)=f(ax+y)=\bold{0}\] by linearity. Additionally, we know that $f(\bold{0})=\bold{0}$. Thus, $ax+y, \bold{0} \in \ker(f)$, so by \ref{thm_subspace} we are done.

  Now suppose that $x,y \in \im(f)$ and $a \in \mathbb{F}$ Then for some $x_{0},y_{0} \in V$, $f(x_{0})=x$ and $f(y_{0})=y$. Therefore, \[af(x_{0})+f(y_{0})=f(ax_{0}+y_{0})=ax+y \in \im(f).\] Furthermore, \[f(\bold{0})=\bold{0}\in \im(f).\]
\end{proof}
\begin{thm}
\label{thm_linspan}
Let $f:V \to W$ be linear and $\beta=\{v_{1}, v_{2}, \dots v_{n}\}$ be a basis for $V$. Then \[\im(f)=\spn(f(\beta)).\]

\end{thm}
\begin{proof}
Let $x \in V$. We have $x=\sum\limits_{i=1}^{n}a_{i}v_{i}$ for $a_{i} \in \mathbb{F}$ and $f(x)=\sum\limits_{i=1}^{n}a_{i}f(v_{i})$. That is, for an arbitrary element $f(x) \in \im(f)$ $f(x) \in \spn(f(\beta))$. The converse containment follows by the same logic.
\end{proof}
\begin{thm}[Dimension Theorem]
\label{thm_dimens}
  Let $f: V \to W$ be linear. Then \[\dim(\ker(f)) + \dim(\im(f))=\dim(V).\]
\end{thm}
\begin{proof}
Let $\{v_{1}, \dots v_{k}\}$ be a basis for $\ker(f)$. Then we may extend this basis to a basis $\{v_{1}, v_{2}, \dots v_{k}, v_{k+1}, \dots v_{n}\}$ for $V$. Now, by \ref{thm_linspan} \[\im(f)=\spn(f(\{v_{1}, \dots v_{n}\}))\] but since $\{v_{1}, \dots v_{k}\} \subseteq \ker(f)$ we have \[\im(f)=\spn(f(v_{k+1}, \dots v_{n})).\] To show that this set is, indeed a basis, for $\im(f)$, suppose that \[\sum_{i=k+1}^{n}a_{i}f(v_{i})=\bold{0}.\] The linearity of $f$ yields \[f(\sum_{i=k+1}^{n}a_{i}v_{i})=\bold{0}\] which is to say that \[\sum_{i=k+1}^{n}a_{i}v_{i} \in \ker(f).\] Thus we may represent this vector in the basis of $\ker(f)$. We have \[\sum_{i=k+1}^{n}a_{i}v_{i}- \sum_{i=1}^{k}b_{i}v_{i}=\bold{0}\] which implies that $a_{i}=0$ because we know that $\{v_{1}, \dots v_{n}\}$ is a basis for $V$. Therefore $\dim(\im(f))=\dim(V)-\dim(\ker(f))$.
\end{proof}
\begin{thm}
\label{thm_inj}
Let $f: V \to W$ be linear. Then $f$ is injective if and only if $\ker(f)=\{\bold{0}\}$.
\end{thm}
\begin{proof}
Suppose that $f$ is injective and that $f(x)=\bold{0}$ for some $x \in V$. We have that $f(x)=f(\bold{0})=\bold{0}$ so $x=\bold{0}$. Conversely, suppose $\ker(f)=\{\bold{0}\}$. Then if $f(x)=f(y)$ we know that $f(x-y)=\bold{0}$, and hence $x-y=\bold{0}$.
\end{proof}
\begin{thm}
\label{thm_injsur}
  Let $f: V \to W$ be linear. If $\dim(V)=\dim(W)$ then the following statements are equivalent:
  \begin{enumerate}
    \item $f$ is injective
    \item $f$ is surjective
    \item $\dim(\im(f))=\dim(V)$
  \end{enumerate}
\end{thm}
\begin{proof}
  Applying, theorem \ref{thm_dimens} and theorem \ref{thm_inj} we have $f$ is injective if and only if $\ker(f)=\{\bold{0}\}$ if and only if $\dim(\ker(f))=0$ if and only if $\dim(\im(f))=\dim(V)=\dim(W)$. And by corollary \ref{cor_bassub} $\im(f)=W$.
\end{proof}
\begin{thm}
\label{thm_uniqmap}
Let $V$ and $W$ be vector spaces,  $\{v_{1},v_{2}, \dots v_{n}\}$ be a basis for $V$ and $\{w_{1},w_{2}, \dots w_{n}\} \subseteq W$. Then there exists a unique linear map $f: V \to W$ such that $f(v_{i})=w_{i}$.
\end{thm}
\begin{proof}
For $x \in V$ let $x=\sum\limits_{i=1}^{n}a_{i}v_{i}$ and define $f:V \to W$ by \[f(\sum\limits_{i=1}^{n}a_{i}v_{i})=\sum\limits_{i=1}^{n}a_{i}w_{i}.\] Trivially $f$ is linear, and if we let $a_{i}=0$ for all $a_{i}\neq a_{j}$ and $a_{j}=1$, we have $f(v_{j})=w_{j}$. To show uniqueness, suppose that there exists another linear function $g:V \to W$ such that $g(v_{i})=w_{i}$. Then for $x \in V$ we have $x=\sum\limits_{i=1}^{n}a_{i}v_{i}$ and $g(x)=\sum\limits_{i=1}^{n}a_{i}g(v_{i})=\sum\limits_{i=1}^{n}a_{i}w_{i}=f(x)$.
\end{proof}
\section{Matrices}
\begin{defn}
\label{defn_orbase}
Let $V$ be a vector space with a basis $\{v_{1},\dots v_{n}\}$. An ordered basis for $V$ is a permutation of the $n-tuple$ $(v_{1}, \dots v_{n})$.
\end{defn}

\begin{defn}
\label{defn_mat}
  Let $f: V \to W$ be a linear function between two vector spaces $V$ and $W$ and let $\beta=(v_{1}, \dots v_{n})$ and $\gamma=(w_{1}, \dots w_{m})$ be ordered bases for $V$ and $W$ respectively. Suppose that $f(v_{j})=\sum\limits_{i=1}^{m}a_{ij}w_{i}$. Then we call the $m \times n$ array with the scalar $a_{ij}$ in the $i^{th}$ row and $j^{th}$ column thereof the matrix representation of $f$ with respect to ordered bases $\beta$ and $\gamma$. We denote this by \[[f]_{\beta}^{\gamma} .\] Furthermore, given $x \in V$ with $x=\sum\limits_{i=1}^{n}b_{i}v_{i}$ we call the $n \times 1$ matrix whose $i^{th}$ row is $b_{i}$ the column vector of $x$ with respect to $\beta$. Analogously we may define the row vector of $x$ with respect to $\beta$.
\end{defn}

\begin{defn}
\label{defn_id}
  $\delta_{ij}:X \to \{0,1\}$ is the map with \[\delta_{ij}=
  \begin{cases}
    1 & i=j \\
    0 & i\neq j
  \end{cases}.\]
$e_{j}$ is the column vector with \[(e_{j})_{ij}=\delta_{ij}.\]
The tuple $(e_{1}, e_{2}, \dots e_{n})$ is called the standard ordered basis for the vector space $\mathbb{F}^{n}$. The $m \times n$ matrix \[[Id_{V}]_{\beta}^{\beta}\] is called the $n \times n$ identity matrix.

\end{defn}
Note that the above definition is well founded as one may easily verify that $\mathbb{F}^{n}$ forms a vector space in the natural way, with vector addition and scalar multiplication defined coordinate-wise.
Additionally, one can easily verify that the $n \times n$ identity matrix is the matrix whose $j^{th}$ column is $e_{j}$.
\begin{thm}
\label{thm_spacelin}
  Let $\mathcal{L}(V,W)$ be the set of all linear functions between two vector spaces $V$ and $W$ over a field $\mathbb{F}$. For $f,g \in \mathcal{L}(V,W)$ and all $x \in V$ define \[f+g=f(x)+g(x)\] and \[af=af(x)\] for all $a \in \mathbb{F}$. Additionally, define $\bold{0}(x)=\bold{0}$. Then $\mathcal{L}(V,W)$ forms a vector space.
\end{thm}
\begin{proof}
Trivially, for any two linear functions $f,g \in \mathcal{L}$ and scalar $a \in \mathbb{F}$ $af+g$ is a linear function.
\end{proof}
\begin{thm}
\label{thm_matsum}
  Let $V$ and $W$ be vector spaces with ordered bases $\beta$ and $\gamma$ respectively, and let $f,g \in \mathcal{L}(V,W)$. Then the following hold:
  \begin{enumerate}
          \item $[f+g]_{\beta}^{\gamma}=[f]_{\beta}^{\gamma}+[g]_{\beta}^{\gamma}$
          \item $[af]_{\beta}^{\gamma}=a[f]_{\beta}^{\gamma}$
  \end{enumerate}
\end{thm}
The proof follows from a direct application of the definition of a matrix.
\begin{defn}
\label{defn_matprod}
  Let $A$ be an $m \times n$ matrix and $B$ be an $n \times p$ matrix, then the product of $A$ and $B$ denoted $AB$ is the $n \times p$ matrix defined by
  \[(AB)_{ij}=\sum_{k=1}^{n}A_{ik}B_{kj}.\]
\end{defn}
\begin{thm}
\label{thm_matprod}
  Let $f: V \to W$ be a linear function between two vector spaces $V$ and $W$ and $g: W \to Z$ be a linear function between $W$ and a vector space $Z$. Let $\alpha \beta \gamma$ be ordered bases for $V W Z$ respectively. Then
  \[[f(g)]_{\alpha}^{\gamma}=[f]_{\alpha}^{\beta}[g]_{\beta}^{\gamma}.\]
\end{thm}
\begin{proof}
  Let $\alpha=(v_{1}, \dots v_{n})$, $\beta=(w_{1}, \dots w_{m})$ and $\gamma=(z_{1}, \dots z_{p})$. We have \[f(g(v_{j}))=f(\sum_{k=1}^{m}B_{kj}w_{k})=\sum_{k=1}^{m}B_{kj}f(w_{k})=\sum_{k=1}^{m}B_{kj}(\sum_{i=1}^{p}A_{ik}z_{i})=\sum_{i=1}^{p}(\sum_{k=1}^{m}A_{ik}B_{kj})z_{i}\]
\end{proof}

\begin{cor}
\label{cor_leftmult}
  Let $A$ be an $m \times n$ matrix over $\mathbb{F}$.
The mapping $L_{A}: \mathbb{F}^{n}\to \mathbb{F}^{m}$ defined by $A \to Ax$ for $x \in \mathbb{F}^{n}$ is linear and \[[L_{A}]_{e}^{e'}=A\] where $e$ and $e'$ are the standard ordered bases for $\mathbb{F}^{n}$ and $\mathbb{F}^{m}$ respectively.
\end{cor}
\begin{proof}
  The linearity of $L_{A}$ follows from theorem \ref{thm_matt} The $j^{th}$ column of $[L_{A}]_{e}^{e'}$ is $L_{A}(e_{j})=Ae_{j}$ which is the $j^{th}$ column of $A$.
\end{proof}

\begin{thm}
\label{thm_matt}
  Let $A$ be an $m \times n$ matrix $B$ and $C$ be $n \times p$ matrices and $D$ and $E$ be $q \times m$ matrices. Then
  \begin{enumerate}
    \item $A(B+C)=AB+AC$ and $(D+E)A=DA+EA$
    \item $a(AB)=(aA)B=A(aB)$
    \item$I_{m}A=A=AI_{n}$
  \end{enumerate}
\end{thm}
The proof follows from a direct application of the definition \ref{defn_matprod}.
\begin{thm}
\label{mat_assc}
  Let $A$ $B$ and $C$ be matrices such that $A(BC)$. Then $AB(C)$ is defined and \[A(BC)=AB(C).\]
\end{thm}
The proof follows by a direct application of matrix multiplication. Analogous results hold for linear functions, all of which follow from the definition of a linear function.
\begin{thm}
\label{thm_matiso}
  Let $V$ and $W$ be vector spaces over $\mathbb{F}$ with dimension $n$ and $m$ respectively. Let $\beta$ and $\gamma$ be ordered bases for $V$ and $W$ respectively. Then there exists an isomorphism between the space $\mathcal{L}(V,W)$ and the space $M_{m \times n}(\mathbb{F})$ of $m \times n$ matrices with entries in $\mathbb{F}$.
\end{thm}
\begin{proof}
  It should be clear that $M_{m \times n}(\mathbb{F})$ is in fact a vector space itself. This space is defined analogously to $\mathbb{F}^{m}$ for an element thereof is nothing but a column vector, and hence a matrix. The \emph{standard} basis for this space is also defined in a similar way, with $1$ in a single entry and $0$ everywhere else.
  By theorem \ref{thm_matsum} this map is linear. By theorem \ref{thm_uniqmap} there exists a unique linear function $T: V \to W$ with $T(v_{i})=w_{i}$ for all $1 \leq i \leq n$. Therefore, there is a unique map $T':V \to W$ such that \[T(v_{i})=\sum_{i=1}^{m}A_{ij}w_{i}.\] That is to say that $[T]_{\beta}^{\gamma}=A$ for some $m \times n$ matrix $A$.
\end{proof}
It follows that we can uniquely associate arrays with matrices, and matrices with linear functions. Thus, we may phrase all of our results on linear functions in terms of multiplying arrays of numbers.
\begin{cor}
\label{thm_matinv}
Let $f: V \to W$ be linear. Then $[f]_{\beta}^{\gamma}$ is invertible if and only if $f$ is invertible. And $([f]_{\beta}^{\gamma})^{-1}=[f^{-1}]_{\gamma}^{\beta}$
\end{cor}
\begin{proof}
  If $[f]_{\beta}^{\gamma}$ is invertible then $[f]_{\beta}^{\gamma}A=A[f]_{\beta}^{\gamma}=I_{n}$ And for some linear function $S$ $B=[S]_{\gamma}^{\beta}$ so
  \[[f]_{\beta}^{\gamma}[S]_{\gamma}^{\beta}=[f(s)]_{\gamma}=I_{n}=[Id_{W}]_{\gamma}\] and \[[S]_{\gamma}^{\beta}[f]_{\beta}^{\gamma}=[S(f)]_{\beta}=I_{n}=[Id_{V}]_{\beta}.\]
  That is, $f(S)=Id_{W}$ and $S(f)=Id_{V}$, whence $S=f^{-1}$.

  Conversely if $f$ is invertible, then \[f^{-1}(f)=Id_{V}\] so \[[f^{-1}(f)]_{\beta}=[Id_{V}]_{\beta}=I_{n}=[f^{-1}]_{\gamma}^{\beta}[f]_{\beta}^{\gamma}.\]Similarly, \[[f]_{\beta}^{\gamma}[f^{-1}]_{\gamma}^{\beta}=I_{n}.\]
\end{proof}
\begin{thm}
  \label{thm_iso}
  Let $V$ and $W$ be vector spaces over a field. Then $V$ is isomorphic to $W$ if and only if $\dim(V)=\dim(W)$.
\end{thm}
\begin{proof}
  If $T:V \to W$ is an isomorphism then $T$ is, by definition, bijective and linear, and hence by theorem \ref{thm_dimens}
  \[\dim(\im(T))=\dim(W)=\dim(V).\] Conversely, if $\dim(V)=\dim(W)$ and $\beta=\{v_{1}, \dots v_{n}\}$ and $\gamma =\{w_{1}, \dots w_{n}\}$. By theorem \ref{thm_uniqmap} there is a unique linear map such that $T(v_{i})=w_{i}$. Furthermore by theorem \ref{thm_spn} \[\im(T)=\spn(T(\beta))=\spn(\gamma)=W.\]
\end{proof}
Therefore, we have shown that an arbitrary vector space is isomorphic to some $\mathbb{F}^{n}$, the cannonical, most intuitive vector space there is, hence demystifying the idea of an abstract vector space. This is a common theme in linear algebra.
\chapter{Linear Systems of Equations}
\section{Rank}
\begin{defn}
\label{defn_elmop}
  An elementary row or column operation on an $m \times n$ matrix $A$ is defined as one of the following:
  \begin{enumerate}
    \item Interchanging any two rows or columns of $A$
    \item Scaling each entry in a row or or column of $A$
    \item Adding a multiple of one row or column to another row or column of $A$
  \end{enumerate}
  An elementary matrix is the result of applying one of the above to the $n \times n$ identity matrix.
\end{defn}
\begin{thm}
\label{thm_elmop}
  Suppose that $B$ is the result of applying an elementary row operation to $A$. Then there exists an elementary matrix $E$ such that $B=EA$. Furthermore, $E$ is the matrix obtained by performing the same elementary row operation to $I_{n}$ as was performed to convert $A$ into $B$. Similarly, if $B$ is the result of applying
  an elementary column operation to $A$, then there exits an elementary matrix $E$ such that $B=AE$, and $E$ is the result of applying the same elementary column operation to $I_{m}$ as was applied to $A$.
\end{thm}
The proof is a tedious verification of cases; the elementary matrices are defined precisely for this to work.
\begin{defn}
\label{defn_rnk}
The rank of a matrix $A$ is defined as the rank of the linear function $L_{A}=Ax$
\end{defn}
\begin{thm}
  \label{thm_subiso}
Let $T: V \to W$ be an isomorphism and $V_{0} \subseteq V$ be a subspace of $V$. Then $T(V_{0})\subseteq W$ is a subspace of $W$. Moreover $\dim(V_{0})=\dim(T(V_{0}))$
\end{thm}
\begin{proof}
  If $V_{0}\subseteq V$ is a subspace of $V$ then $T(V_{0})$ is a subspace of $W$ because $T$ is linear. Further, we may consider the map $T':V_{0} \subseteq T(V_{0})$ such that $T'(x)=T(x)$ for all $x \in V_{0}$. By theorem \ref{thm_dimens} we have \[\dim(\ker(T'))+\dim(\im(T'))=0+ \dim(T(V_{0}))=\dim(V_{0}).\]
\end{proof}
\begin{thm}
\label{rnkeq}
  Let $T: V \to W$ be linear and $A=[T]_{\beta}^{\gamma}$. Then $\rank(T)=\rank(L_{A})$
\end{thm}
\begin{proof}
  Consider the map $\phi_{\beta}:V \to \mathbb{F}^{n}$. That is, the function mapping a vector to its representation in coordinates. This is linear by definition and invertible as we know that any basis represents a vector uniquely as a linear combination of its elements. We have
  \[L_{A}(\mathbb{F}^{n})=L_{A}\phi_{\beta}(V)=\phi_{\gamma}(T(V)).\] It follows, by theorem \ref{thm_subiso},  that \[\dim(\im(L_{A}))=\dim(\im(T))\] because $\phi_{\gamma}$ is an isomorphism.
\end{proof}
\begin{thm}
\label{thm_rnkprp}
  Let $A$ be an $m \times n$. Let $P$ and $Q$ be invertible $m \times m$ and $n \times n$ matrices, respectively.
  Then
  \begin{enumerate}
    \item $\rank(AQ)=\rank(A)$
    \item $\rank(PA)=\rank(A)$
    \item $\rank(PAQ)$
  \end{enumerate}
  \begin{proof}
    \begin{align}
      \im(L_{AQ})&=\im(L_{A}L_{Q}) \\
                 &= L_{A}L_{Q}(\mathbb{F}^{n}) \\
                 &= L_{A}(L_{Q}((\mathbb{F}^{n})) \\
                 &= L_{A}(\mathbb{F}^{n}) \\
                 &=\im(L_{A})
    \end{align}
    Thus, $\rank(L_{AQ})=\rank(L_{A})$.
    Similarly, $\im(L_{P}L_{A})=L_{P}(\im(L_{A}))=\im(L_{A})$
    and so $\dim(\im(L_{P}L_{A}))=\dim(\im(L_{A}))$ since $P$ is an isomorphism.
    It follows, by applying the previous two results that
    $\rank(PAQ)=\rank(A)$.
  \end{proof}
\end{thm}
\begin{thm}
\label{thm_colm}
Let \[A=\begin{pmatrix}
          a_{11} & \cdots & a_{1n} \\
          \vdots & \ddots & \vdots \\
          a_{1m} & \cdots & a_{mn}

        \end{pmatrix}.\] Then $\rank(A)=\dim \left(
        \spn{ \left \{
          \begin{pmatrix} a_{11} \\ \vdots \\ a_{1m}\end{pmatrix}, \cdots \begin{pmatrix} a_{1n}\\ \vdots \\ a_{mn} \end{pmatrix}
        \right \}
        }\right )
        $
\end{thm}
\begin{proof}
  \begin{align}
    \im(L_{A}) &= L_{A}(\mathbb{F}^{n}) \\
               &= L_{A}(\spn{\{e_{1}, \dots e_{n}\}}) \\
               &= \spn{\{Ae_{1}, \dots, Ae_{n}\}} \\
    &=
        \spn{ \left \{
          \begin{pmatrix} a_{11} \\ \vdots \\ a_{1m}\end{pmatrix}, \cdots \begin{pmatrix} a_{1n}\\ \vdots \\ a_{mn} \end{pmatrix}
        \right \}
        }
  \end{align}
  Furthermore, $\dim(\spn{(X)})$ is nothing but the number of linearly independent vectors in $X$ for any set of vectors $X$. Thus we have shown
  that the rank of a matrix is nothing but the number of linearly independent vectors in its columns.
\end{proof}
\begin{thm}
\label{thm_rrefrnk}
Let $A$ be an $m \times n$ matrix. Then a finite composition of elementary row and column operations applied to $A$ results in a matrix
of the form
\[ \begin{pmatrix}
     I_{\rank(A)} & O_{1} \\
     O_{2} & O_{3}
   \end{pmatrix}
 \]
 where $O_{1}, O_{2}, O_{3}$ are zero matrices.
\end{thm}
\begin{proof}
  First, note that if $A$ is a zero matrix, then by theorem \ref{thm_colm} $\rank(A)=0$, and so $A=I_{0}$, the degenerate case of our claim.
  Suppose otherwise.
  We proceed by induction on $m$, the number of rows of $A$. In the case that $m=1$, we may convert $A$ to a matrix of the form
  \[\begin{pmatrix}
      1 & 0 & \cdots & 0
    \end{pmatrix}\] by first making the leftmost entry $1$ and adding the corresponding additive inverses of the others to the other columns.
  Clearly the rank of the above matrix is $1$ and is of the form
  \[\begin{pmatrix}
I_{1} & O
    \end{pmatrix}\]
  This is another degenerate case, as it lacks zeros below the identity.
  Now suppose that our theorem holds when $A$ has $m-1$ rows.

  To demonstrate that our theorem holds when $A$ is an $m \times n$ matrix, notice that when $n=1$, we can argue that our theorem holds as before, but
  using row operations instead of column operations. This is another degenerate case. For $n>0$, note that there exists an entry $A_{ij}\neq0$
  and by applying at most an elementary row and column operation, we can move $A_{ij}$ to position $1,1$. Additionally, we may transform $A_{ij}$ to
  value $1$, and as before, transform all of the entries in row and column 1 besides $A_{ij}$ to $0$. Thus we have a matrix of the form
  \[\begin{pmatrix}
      1 & 0 & \cdots & 0 \\
      0 & x_{11} & \cdots & x_{1 \ n-1}       \\
      \vdots & \vdots & \ddots &  \vdots  \\
      0 & x_{m-1 \ 1}& \cdots & x_{m-1 \ n-1}
    \end{pmatrix}\]
\end{proof}
The submatrix defined by $x_{ij}$ is of dimension $m-1 \times n-1$ and so must have rank $\rank(A)-1$ as elementary opertations preserve rank and deleting a row and column of a matrix reduces its rank by 1. Furthermore, by our induction hypothesis the above matrix may be converted via a finite number
of elementary operations to a matrix of the form
\[\begin{pmatrix}
    1 & 0 & \cdots & 0 \\
    0 & I_{\rank(A)-1} & O_{1} \\
    \vdots &  &   \\
    0 & O_{2} & O_{3}
  \end{pmatrix}\]
Therefore, for an $m \times n$ matrix $A$, a finite number of elementary operations converts it into a matrix of the form
\[\begin{pmatrix}
    I_{\rank(A)} & O_{1} \\
    O_{2} & O_{3} \\
  \end{pmatrix}\].
\begin{thm}
\label{thm_colrow}
  For any matrix $A$, $\rank(A^{T})=\rank(A)$.
\end{thm}
\begin{proof}
  By theorem \ref{thm_rrefrnk}, we may convert $A$ to a matrix $D=BAC$ where $B=E_{1}\cdots E_{p}$ and $C=G_{1}\cdots G_{q}$ where $E_{i}$ and $G_{i}$ are elementary row and column matrices respectively. It follows that $D^{T}=C^{T}A^{T}B^{T}$, whence $\rank(A^{T})=\rank(D^{T})$ by theorem (insert) because elementary
  matrices are invertible, and so is the transpose of the compositions thereof. Further, $D^{T}$ must be of the same form as $D$ since the only nonzero entries of $D$ are along the diagonal from entry $1,1$ to entry $\rank(A), \rank(A)$. Hence, we have $\rank(A)$ linearly independent columns in the matrix $D^{T}$.

  Since the columns of $D^{T}$ are the rows of $D$, we see that the number of linearly independent columns of $A$ is equal to the number of linearly
  independent columns of $A^{T}$. In other words, the dimension of the space generated  by the columns of $A$ is equal to the dimension of the space generated by its rows.
\end{proof}
\begin{thm}
\label{thm_invm}
  Let $A$ be an invertible $n \times n$ matrix. Then $A$ is a product of elementary matrices.
\end{thm}
\begin{proof}
  By the dimension theorem, if $A$ is invertible,  then $\rank(A)=n$. So by theorem \ref{thm_rrefrnk} $A$ may converted into a matrix of the form
  $I_{n}=E_{1}\cdots E_{p}AG_{1} \cdots G_{q}$, whence $A=E_{1}^{-1}\cdots E_{p}^{-1}I_{n}G_{1}^{-1}\cdots G_{q}^{-1}$.
\end{proof}
\begin{thm}
\label{thm_rnkinq}
  Let $T:V \to W$ and $U:W \to Z$.  Then
  \begin{enumerate}
    \item $\rank(TU)\leq \rank(U)$
    \item $\rank(TU) \leq \rank(T)$
  \end{enumerate}
\end{thm}
\begin{proof}
  We have
  \begin{align}
    \rank(TU)&=\dim(\im(TU)) \\
            &= \dim(\im(T(U(V)))) \\
            &\subseteq U(W) \\
            &= \im(U)
  \end{align}
  Therefore, $\dim(\im(TU))\leq \dim(\im(U))$. Next, let $\beta, \gamma, \phi$ be ordered bases for $V, W, $ and $Z$, respectively; and let
  $A=[T]_{\beta}^{\gamma}$ and $B=[U]_{\gamma}^{\phi}$.
  By theorem \ref{thm_colrow}
  \begin{align}
    \dim(\im(TU))&=\dim(\im(AB)) \\
                 &= \dim(\im((AB)^{T}) \\
                 &= \dim(\im(B^{T}A^{T})) \\
                 &\leq \dim(\im(A^{T})) \\
                 &= \dim(\im(A)) \\
    &= \dim(\im(T))
  \end{align}
\end{proof}
\section{Form}
We now apply the fruits of our investigation into vector spaces and linearity to solve systems of linear equations.
\begin{defn}
\label{def_linsys}
  A linear system of equations is a collection of $m$ equations of the form:
  \[ a_{1}x_{1}+ \cdots +a_{n}x_{n}=b \] where $a_{i}, x_{i},b \in \mathbb{F}$ for $1 \leq i \leq n$.
  Equivalently, we may say
  $Ax=b$ for an $m \times n$ matrix $A$, where $x= \begin{pmatrix} x_{1} \\ \vdots \\ x_{n}\end{pmatrix}$ and $b= \begin{pmatrix} b_{1} \\ \vdots \\ b_{m}\end{pmatrix}$.
  If $b=\bold{0}$, the linear system is said to be homogenous.
\end{defn}
\begin{defn}

A solution to a linear system is a vector $s \in \mathbb{F}^{n}$ such that $As=b$
\end{defn}

\begin{thm}
\label{thm_homsoln}
Let $A$ be an $m \times n$ matrix over $\mathbb{F}$. If $m<n$, then the homogenous system $Ax=0$ has a nontrivial solution.
\end{thm}
\begin{proof}
  Notice that, the solution set to the system $Ax=0$ is $\ker(L_{A})$, so by the dimension theorem, $\dim(\ker(A))=n-\rank(L_{A})$. Additionally,
  we know that $\rank(A)$ is nothing but the number of linearly independent vectors defined by its rows which certainly cannot exceed $m$. Therefore
  $\rank(A) \leq m < n$, in which case $n-\rank(A)=\dim(\ker(A))>0$, and so $\ker(A)\neq \{0 \}$.
\end{proof}
\begin{thm}
\label{thm_gensoln}
For any solution $s$ to the linear system $Ax=b$, \[\{s+s_{0}:As_{0}=\bold{0} \}\] is its solution set.
\end{thm}
\begin{proof}
Suppose that $As=b$ and $As'=b$. Then $A(s'-s)=As'-As=b-b=0$. It follows that $s+(s'-s) \in S$. Conversely, if $y \in S$, then $y=s+s'$, in which case $Ay=A(s+s')=As+As'=b+0=b$. That is, $Ay=b$.
\end{proof}
\begin{thm}
\label{thm_uniqsoln}
  Let $Ax=b$ for an $n \times n$ matrix $A$. If $A$ is invertible, then the system has a single solution $A^{-1}b$. If the system has a single solution, then $A$ is invertible.
\end{thm}
\begin{proof}
Suppose $A$ is invertible. Then $A(A^{-1}b)=AA^{-1}(b)=b$. Furthermore, if $As=b$ for some $s \in \mathbb{F}^{n}$, then $A^{-1}(As)=A^{-1}b$ and so $s=A^{-1}b$.
Next, suppose that the system has a unique solution $s$. Then by theorem \ref{thm_gensoln}, we know that the solution set $S=\{s+s_{0}: As_{0}=0 \}$. But this is only the case if $\ker(A)=\{0\}$, lest $s$ not be unique. And so, by the dimension theorem, $A$ is invertible.
\end{proof}
\begin{thm}
\label{thm_nonemsoln}
The linear system $Ax=b$ has a nonempty solution set if and only if $\rank(A)=\rank(A|b)$.
\end{thm}
\begin{proof}
  If the system has a solution, then $b \in \im(L_{A})$. Additionally, $\im(L_{A})=L_{A}(F^{n})$ and $L_{A}(e_{i})=Ae_{i}=
  \begin{pmatrix} a_{1i} \\ \vdots \\ a_{ni}   \end{pmatrix}$. Therefore, since $L_{A}(\mathbb{F}^{n})= \spn \{Ae_{1}, \dots Ae_{n}\}$,
  $\im(L_{A})=\spn\{A_{1}, \dots A_{n}\}$, where $A_{i}$ is the $i^{th}$ column of $A$.
  Certainly, $b \in \spn\{A_{1}, \dots A_{n}\}$ if and only if $\spn\{A_{1}, \dots A_{n}\}=\spn\{A_{1}, \dots A_{n}, b\}$, which is to say
  $\dim(\im(\spn\{A_{1}, \dots A_{n}\}))=\dim(\im(\spn\{A_{1}, \dots A_{n}, b\}))$, or, $\rank(A)=\rank(A|b)$.
\end{proof}
\begin{cor}
\label{cor_infsoln}
Let $Ax=b$ be a linear system of $m$ equations in $n$ variables. Then its solution set is either, empty, of one element, or of infinitely many elements (provided that $\mathbb{F}$ is not a finite field).
\end{cor}
\begin{proof}
  By theorem \ref{thm_nonemsoln} $Ax=b$ has a nonempty solution set if and only if $\rank(A)=\rank(A|b)$. Therefore, it may be that our linear system has no solutions; however, supposing that this is not the case, by theorem \ref{thm_uniqsoln} it has a unique solution if and only if $A$ is invertible. Finally, assume that our linear system has neither no solution nor a single solution. This yields
  \begin{equation}
Ax_{1}=Ax_{2}=b
\end{equation}
for $x_{1}, x_{2} \in \mathbb{F}^{n}$, which implies
\begin{align}
  Ax_{1}-Ax_{2}&=\bold{0} \\
               &=A(x_{1}-x_{2}) \\
               &= nA(x_{1}-x_{2}) \\
               &= A(n(x_{1}-x_{2})) \\
\end{align}
where $n \in \mathbb{F}$.
Thus, by theorem \ref{thm_gensoln}
\[A(x_{1}+n(x_{1}-x_{2}))=b.\]
\end{proof}
\section{Solution}
\begin{defn}
\label{defn_rref}
A matrix of the form \[\begin{pmatrix} a_{11} & \cdots & a_{1n}\\ \vdots & \ddots & \vdots \\a_{m1} & \cdots & a_{mn}\end{pmatrix}\] is said to be in reduced echelon form if
\begin{enumerate}
  \item $a_{ii}\neq 0$ implies that $a_{ij}=1$
  \item $a_{ij} \neq 1$ implies that $a_{ij}=0$
  \item $a_{ij}=0$ for all $1\leq j \leq n$ implies that $i < r$ for all nonzero rows $\begin{pmatrix}a_{r1} & \cdots & a_{rn}\end{pmatrix}$
\end{enumerate}
\end{defn}
\begin{thm}
\label{thm_rref}
Any matrix can be converted into reduced echelon form via a finite number of elementary row operations.
\end{thm}
\begin{proof}
This is a restatement of theorem \ref{thm_rrefrnk}.
\end{proof}
This form is of particular interest because reducing an augmented matrix is equivalent to solving a linear system of equations. We now have a
procedure for solving arbitrary systems of linear equations. For example, we may now demonstrate that a set of vectors is linearly dependent by
finding a nontrivial solution to a linear system of equations; similarly we may apply theorem \ref{thm_nonemsoln} to demonstrate that a set of vectors is linearly dependent. In the following chapter, we will also see that computing the elements of an eigenspace is made possible by reducing a matrix.
It follows that
\begin{cor}
\label{cor_soln}
For any invertible $n \times n$ matrix $A$.
\[A^{-{1}}(A|I_{n}) = E_{1} \cdots E_{p}(A|I_{n})=(I_{n}|A^{-1})\]
where $E_{1}, \dots, E_{p}$ are elementary matrices.
\end{cor}
Notice that the above elementary matrices may be either row or column matrices; however, since we are left multiplying, the product will result in a
row operation.
Thus we now have a procedure for finding the inverse of any matrix: perform row operations to convert it into the identity matrix, while accounting for each change.
Additionally,
\begin{cor}
\label{cor_solninvar}
Let $A$ be an $m \times n$ matrix and $C$ be an invertible $n \times n$ matrix. Then the solutions sets to the linear systems
\[Ax=b \text{and} CAx=Cb \] are equal.
\end{cor}
This follow directly from the invertibility, and fits with our intuition: as we row reduce a linear system, its solutions do not change.
\chapter{The Determinant}
\section{Permuations}
define determinant
show equal to cofactor expansion
\section{Cofactor Expansion}
deduce enough properties to define the determinat more formally
\section{Multilinear and Alternating}
demonstrate cofactor expansion is unqiue multilinear alternating etc
hence permutation=cofactor=unique such function
\section{Properties}
deduce remaining important properties
need invertible iff det nonzero
\section{Measure}
\chapter{Eigenspaces}
\section{Characteristic Polynomial}
\section{Diagonalization and Similarity}
\section{Dimension}
\chapter{Orthogonality}
\section{Inner Products}
Hello

\section{The Adjoint}
\section{Orthogonal Projections}
\begin{defn}
\label{orthcomp}
Let $W \subseteq V$. The orthogonal complement of $W$ is defined as $W^{\perp}=\{v \in V : \langle v, w \rangle=0 \text{ for all } w \in W\}$.
\end{defn}

\begin{thm}
\label{thm_orthodecomp}
  Let $W \subseteq V$.Then for any $x \in V$ there exist unique vectors $x_{w} \in W$ and $x^{\perp} \in W^{\perp}$ such that $x=x_{W}+x^{\perp}$. In other words $V=W \oplus W^{\perp}$.
\end{thm}
\begin{proof}
  Let $\{w_{1}, \dots w_{n}\}$ be an orthonormal basis for $W$ $x_{W}=\sum_{i=1}^{n}\langle x, w_{i} \rangle w_{i}$ and $x^{\perp}=x-x_{W}$. Certainly $x_{W} \in W$ and $x=x_{W}+x^{\perp}$. To show that $x^{\perp} \in W^{\perp}$ we have
  \begin{align}
    \langle x^{\perp},w_{j}\rangle &= \langle x-x_{W}, w_{j} \rangle \\
                   &= \langle x- \sum_{i=1}^{n} \langle x, w_{i} \rangle w_{i}, w_{j} \rangle \\
                   &= \langle x, w_{j} \rangle - \sum_{i=1}^{n}\langle x, w_{i} \rangle \langle w_{i}, w_{j} \rangle \\
    &= 0
  \end{align}
  For uniqueness, suppose that $x=y+z$ for $y \in W$ and $z \in W^{\perp}$. Then $x_{W}+x^{\perp}=y+z$ and so \[x_{W}-y=z-x^{\perp} \in W \cap W^{\perp}.\]
  But $W \cap W^{\perp}=\{\bold{0}\}$ so $x_{W}=y$ and $x^{\perp}=z$.
\end{proof}
\begin{cor}
  \label{cor_min}
For all $y \in W$ \[||x-x_{W}||\leq ||x-y||\]
\end{cor}
\begin{proof}
\begin{align}
  ||x-y||^{2} &= ||x_{W}+x^{\perp}-y||^{2} \\
              &=||(x_{W}-y)+x^{\perp}||^{2} \\
              &= ||x_{W}-y||^{2}+||x^{\perp}||^{2} \\
              &\geq ||x^{\perp}||^{2} = ||x-x_{W}||.
\end{align}
\end{proof}
\begin{thm}
The following statments are true
\begin{enumerate}
  \item $W^{\perp}$ is a subspace of $V$
  \item $\dim(W^{\perp})=\dim(V)-\dim(W)$
\end{enumerate}
\end{thm}
\begin{proof}
  Firstly, note that $ \langle \bold{0}, w\rangle=\bold{0}$ for all $w \in W$, so $\bold{0} \in W^{\perp}$. Furthermore, if $\langle w, c\rangle=0$ for some $w \in W$ then
  $\langle aw, c \rangle = a \langle w, c \rangle =0 $ by linearity. Similarly, if $\langle w, a \rangle=0  $ and $\langle b, c \rangle=0  $ then $\langle w, a \rangle+ \langle b, c \rangle= \langle w+b, c \rangle =0$. Secondly, $V= W^{\perp} \oplus W$ implies that $\dim(V)=\dim(W^{\perp})+\dim(W)$.
\end{proof}

\begin{thm}
Let $W \subseteq V$ $x \in V$ and $\beta=\{v_{1}, \dots v_{n}\}$ be an orthonormal basis for $W$ and $A$ be the matrix whose $j^{th}$ column is $v_{j}$. Then the orthogonal projection of $x$ on $W$ $x_{W}=AA^{*}x$.
\end{thm}

\begin{proof}
  We begin by demonstrating that $W^{\perp}=\ker A^{*}$. We have
  \[A^{*}x= \begin{pmatrix}v_{1}^{*}x \\ \vdots \\ v_{n}^{*}x \end{pmatrix}=\begin{pmatrix}\langle v_{1}, x \rangle \\  \vdots \\ \langle v_{n}, x \rangle\end{pmatrix}.\] Certainly
  $A^{*}x=\bold{0}$ if and only if $\langle v_{i}, x \rangle=0$ for all $1 \leq i \leq n$. But that is to say $x \in W^{\perp}$, and so \[\ker(A^{*})=W^{\perp}.\]
  Let $x=x_{W}+x_{\perp}$ be the orthogonal decomposition of $x$ with respect to $W$.
  Note that $\im(A)=\spn\beta$. Therefore, for some vector $c$ $Ac=x_{W}$, which means that $x-x_{W}=x-Ac \in W^{\perp}$. It follows that
  $A^{*}(x-Ac)=0$ and so \[A^{*}Ac=A^{*}x.\] Thus, we see that $x_{W}=Ac$. Furthermore, since $\beta$ is orthonormal, $A$ must be unitary, in which case \[Ac=AA^{*}x=x_{W}.\]
\end{proof}
\begin{cor}
  \label{cor_orthproj}
$AA^{*}$ is the unique projection of $V$ on $W=\{x \in V: AA^{*}x=x\}$ along $W^{\perp}=\ker(AA^{*})$. Additionally, $AA^{*}$ is self adjoint.
\end{cor}
\begin{proof}
  Surely $AA^{*}$ is linear, and since we know that $x=x_{W}+x_{W^{\perp}}$ for all $x \in V$ it follows that $(AA^{*})^{2}x=AA^{*}x_{W}=x_{w}=AA^{*}x$. Thus the orthogonal projection is, in fact, a projection on $W=\{x \in V: AA^{*}x=x \}$ along $W^{\perp}=\ker(AA^{*})$, by theorem \ref{thm_projiff} ($V=W \oplus W^{\perp}$). Since we know that the orthogonal decomposition of a vector with respect to a given subspace is unique, it follows that $AA^{*}$ is unique.
  Furthermore,
  \begin{align}
    (AA^{*})^{*}&=(A^{*})^{*}A^{*} \\
    &=AA^{*}.
  \end{align}
\end{proof}
\section{Normal and Unitary Operators}
self adjoint iff orthogonal projection
all unitary operators are rotations
\section{Definiteness}
\chapter{Matrix Decomposition}
\section{Schur's Theorem}
\section{Spectral Theorem}
\section{Singular Value Decomposition and Pseudo-inverse}
\appendix
\chapter{Set Theory}
Axiom of choice
\chapter{The Complex Field}
fundamental theorem of algebra
\chapter{Block Matrices}
need to prove result for diagonalization proof
\chapter{Multilinearity and Sesquilinearity}
pos definite matrices generate inner products uniquely
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{References}
\printbibliography[title={References}]





\end{document}
